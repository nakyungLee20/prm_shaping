{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a46e074b",
   "metadata": {},
   "source": [
    "# Import Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66cd026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerBase,\n",
    "    BitsAndBytesConfig,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a67d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PRMConfig:\n",
    "    \"\"\"Configuration class for PRM hyperparameters and settings\"\"\"\n",
    "    # MC config\n",
    "    model_name:             str = \"Qwen/Qwen2.5-Math-7B\"    # \"Qwen/Qwen2.5-Math-7B\", \"Qwen/Qwen2.5-Math-7B-Instruct\" , \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", \"meta-llama/Llama-3.1-8B\"\n",
    "    max_new_tokens:         int = 512\n",
    "    num_rollouts:           int = 8      \n",
    "    samples_per_question:   int = 1\n",
    "    use_llm:                bool = True  # Use llm for masking\n",
    "    reward_type:            str = \"contri\"  # ori, contri, mi, naive, norm\n",
    "    # PRM Model config \n",
    "    hidden_size:        int = 512      # 256-1024 범위에서 적절\n",
    "    num_layers:         int = 3        # 2-4 범위에서 적절\n",
    "    dropout:            float = 0.2    # 0.1-0.3 범위에서 적절\n",
    "    # PRMTrainer config \n",
    "    batch_size:         int = 16       # 12 → 16으로 증가 (더 안정적)\n",
    "    learning_rate:      float = 3e-4   # 5e-4 → 3e-4로 감소 (더 안정적)\n",
    "    num_workers:        int = 4        # 적절\n",
    "    weight_decay:       float = 1e-2   # 적절\n",
    "    lr_scheduler:       str = \"cosine\" # 적절\n",
    "    dataset_size:       int = 0\n",
    "    warmup_steps:       int = 40       # 22 → 50으로 증가 (더 안정적)\n",
    "    grad_clip:          float = 1.0    # 적절\n",
    "    epochs:             int = 20       # 25 → 15로 감소 (early stopping 고려)\n",
    "    # Misc config\n",
    "    use_wandb:          bool = True\n",
    "    wandb_project:      str = \"mc_prm\"\n",
    "    run_name:           str = \"test_400_0715\"\n",
    "    checkpoint_dir:     str = \"./checkpoints/0715/contri\"\n",
    "    seed:               int = 42\n",
    "\n",
    "class ProcessRewardModel(nn.Module):\n",
    "    \"\"\"Enhanced Process Reward Model with dropout and layer normalization\"\"\"\n",
    "    def __init__(self, input_size: int, cfg: \"PRMConfig\"):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            input_size : CLS-embedding dim of the frozen LLM backbone\n",
    "            cfg        : PRMConfig instance (hidden_size, num_layers, dropout …)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        # self.output_size = cfg.output_size\n",
    "        h = cfg.hidden_size\n",
    "        p_drop = cfg.dropout\n",
    "        n_layers = cfg.num_layers\n",
    "        act_fn     = nn.GELU()\n",
    "\n",
    "         # ── first projection ────────────────────────────────────────────\n",
    "        self.in_proj = nn.Sequential(\n",
    "            nn.Linear(input_size, h),\n",
    "            nn.LayerNorm(h),\n",
    "            act_fn,\n",
    "            nn.Dropout(p_drop),\n",
    "        )\n",
    "\n",
    "        # ── stacked residual blocks ─────────────────────────────────────\n",
    "        blocks = []\n",
    "        for _ in range(n_layers - 1):\n",
    "            blocks.append(\n",
    "                nn.Sequential(                   # pre-LN residual MLP\n",
    "                    nn.LayerNorm(h),\n",
    "                    nn.Linear(h, h),\n",
    "                    act_fn,\n",
    "                    nn.Dropout(p_drop),\n",
    "                    nn.Linear(h, h),\n",
    "                    nn.Dropout(p_drop),\n",
    "                )\n",
    "            )\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "        # ── output head ────────────────────────────────────────────────\n",
    "        self.out_proj = nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            nn.Linear(h, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.in_proj(x)\n",
    "        for blk in self.blocks:\n",
    "            x = x + blk(x)          # residual connection\n",
    "        return self.out_proj(x).squeeze(-1)\n",
    "\n",
    "    def get_complexity(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n",
    "class StepwisePRMDataset(Dataset):\n",
    "    \"\"\"mcr rewards가 반환한 entries(list[dict])를 (input_ids, scalar_reward) 샘플들로 변환한다.\n",
    "    한 entry = {question, completion[steps], rewards[float], …} →  (Problem + Step1, r1), (Problem + Step1 \\nStep2, r2) …\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        entries: List[dict],\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        max_length: int = 512,\n",
    "        reward_type: str = \"naive\",\n",
    "        *,\n",
    "        cache_encodings: bool = True,\n",
    "        preprocess: bool = True,\n",
    "    ):\n",
    "        self.tokenizer   = tokenizer\n",
    "        self.max_length  = max_length\n",
    "        self.reward_type = reward_type\n",
    "        self.cache       = {} if cache_encodings else None\n",
    "        self.samples: List[Tuple[str, float]] = []\n",
    "\n",
    "        for e in entries:\n",
    "            q_txt   = e[\"question\"]\n",
    "            steps   = e[\"completion\"]\n",
    "            ans = e[\"gold_answer\"]\n",
    "            o_rewards = e[\"ori_rewards\"]\n",
    "            assert len(steps) == len(o_rewards)\n",
    "\n",
    "            if self.reward_type == \"contri\":\n",
    "                rewards = e[\"contributions\"]\n",
    "                # rewards = [max(0.0, x) for x in contri]\n",
    "            elif self.reward_type == \"mi\":\n",
    "                rewards = e[\"mi_rewards\"]\n",
    "            elif self.reward_type == \"naive\":\n",
    "                rewards = e[\"naive_rewards\"]\n",
    "            else:\n",
    "                rewards = o_rewards\n",
    "\n",
    "            prefix_lines = [f\"Problem: {q_txt}\"]\n",
    "            for step_txt, r in zip(steps, rewards):\n",
    "                prefix_lines.append(step_txt)\n",
    "                full_txt = \"\\n\".join(prefix_lines)\n",
    "                if preprocess:\n",
    "                    full_txt = self._clean(full_txt)\n",
    "                self.samples.append((full_txt, float(r)))   # (text, reward)\n",
    "\n",
    "    # --------------------------------------------------------------------- utils\n",
    "    @staticmethod\n",
    "    def _clean(txt: str) -> str:\n",
    "        \"\"\"whitespace normalize + 소문자화(선택적) 등 간단 전처리\"\"\"\n",
    "        txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "        return txt\n",
    "\n",
    "    # --------------------------------------------------------------------- dunder\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, reward = self.samples[idx]\n",
    "\n",
    "        if self.cache is not None and text in self.cache:\n",
    "            ids = self.cache[text]\n",
    "        else:\n",
    "            ids = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids.squeeze(0)\n",
    "            if self.cache is not None:\n",
    "                self.cache[text] = ids\n",
    "\n",
    "        return ids, torch.tensor(reward, dtype=torch.float32)\n",
    "\n",
    "class PRMTrainer:\n",
    "    \"\"\"\n",
    "    (1) entries(list[dict]) → StepwisePRMDataset\n",
    "    (2) LLM encoder + PRM head fine-tuning\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: PRMConfig, model, tokenizer):\n",
    "        self.cfg = cfg\n",
    "        torch.manual_seed(cfg.seed)\n",
    "\n",
    "        # ----------------------------- Backbone model LLM (frozen or fine-tuned)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model  = model\n",
    "        self.model.eval()       # LLM은 feature extractor로 freeze\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        feat_dim = self.model.config.hidden_size\n",
    "        self.prm = ProcessRewardModel(feat_dim, cfg=cfg)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.prm.to(self.device)\n",
    "\n",
    "        self.opt  = optim.AdamW(self.prm.parameters(), lr=cfg.learning_rate, weight_decay = cfg.weight_decay)\n",
    "        self.crit = nn.MSELoss()\n",
    "        # self.crit = nn.BCELoss()\n",
    "\n",
    "        self.scheduler = None\n",
    "        if cfg.lr_scheduler == \"cosine\":                   \n",
    "            # total steps = (#batches per epoch) × epochs\n",
    "            self.total_steps = math.ceil(cfg.epochs * cfg.dataset_size / cfg.batch_size)\n",
    "            def lr_lambda(step):\n",
    "                if step < cfg.warmup_steps:\n",
    "                    return step / max(1, cfg.warmup_steps)\n",
    "                progress = (step - cfg.warmup_steps) / max(1, self.total_steps - cfg.warmup_steps)\n",
    "                return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "            self.scheduler = LambdaLR(self.opt, lr_lambda)\n",
    "        elif cfg.lr_scheduler == \"linear\":\n",
    "            # Linear warmup + decay\n",
    "            self.total_steps = math.ceil(cfg.epochs * cfg.dataset_size / cfg.batch_size)\n",
    "            def lr_lambda(step):\n",
    "                if step < cfg.warmup_steps:\n",
    "                    return step / max(1, cfg.warmup_steps)\n",
    "                return max(0.0, (self.total_steps - step) / (self.total_steps - cfg.warmup_steps))\n",
    "            self.scheduler = LambdaLR(self.opt, lr_lambda)\n",
    "        elif cfg.lr_scheduler == \"step\":\n",
    "            # Step decay\n",
    "            self.scheduler = optim.lr_scheduler.StepLR(self.opt, step_size=5, gamma=0.5)\n",
    "\n",
    "        self.ckpt_dir = Path(cfg.checkpoint_dir)\n",
    "        self.ckpt_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        self.wandb_run = None\n",
    "        if cfg.use_wandb:                                  # <-- config에 플래그\n",
    "            self.wandb_run = wandb.init(\n",
    "                project=cfg.wandb_project,                 # e.g. \"omega-prm\"\n",
    "                name=cfg.run_name,                         # e.g. \"qwen7b-prm\"\n",
    "                config=vars(cfg),                          # 모든 하이퍼파라미터 로깅\n",
    "            )\n",
    "\n",
    "    # ----------------------------------------------------------------- features\n",
    "    @torch.no_grad()\n",
    "    def _encode(self, ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"input_ids [B,T] → [B, feat_dim] using 마지막 hidden state의 CLS-like 첫 토큰\"\"\"\n",
    "        out = self.model(input_ids=ids, return_dict=True,output_hidden_states=True)\n",
    "        features = out.hidden_states[-1][:, 0, :]     # CLS embedding\n",
    "        return features.float()\n",
    "\n",
    "    # ----------------------------------------------------------------- loop util\n",
    "    def _run_epoch(self, loader: DataLoader, train: bool, epoch_idx: int) -> float:\n",
    "        self.prm.train(train)\n",
    "        total = 0.0\n",
    "        num_batches = len(loader)\n",
    "        \n",
    "        for step, (ids, reward) in enumerate(loader):\n",
    "            ids, reward = ids.to(self.device), reward.to(self.device)\n",
    "\n",
    "            with torch.set_grad_enabled(train):\n",
    "                feats  = self._encode(ids)\n",
    "                pred   = self.prm(feats).squeeze(-1)\n",
    "                loss   = self.crit(pred, reward)\n",
    "                \n",
    "                if train:\n",
    "                    self.opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(self.prm.parameters(), self.cfg.grad_clip)\n",
    "                    # Gradient accumulation (optional)\n",
    "                    if hasattr(self.cfg, 'grad_accum_steps') and self.cfg.grad_accum_steps > 1:\n",
    "                        if (step + 1) % self.cfg.grad_accum_steps == 0:\n",
    "                            self.opt.step()\n",
    "                            if self.scheduler: self.scheduler.step()\n",
    "                    else:\n",
    "                        self.opt.step()\n",
    "                        if self.scheduler: self.scheduler.step()\n",
    "\n",
    "            total += loss.item()\n",
    "\n",
    "            # -------- minibatch logging --------\n",
    "            if self.wandb_run and train:\n",
    "                wandb.log({\n",
    "                    \"batch_loss\": loss.item(),\n",
    "                    \"epoch\": epoch_idx + step / num_batches,\n",
    "                    \"lr\": self.opt.param_groups[0][\"lr\"],\n",
    "                    \"grad_norm\": sum(p.grad.data.norm(2).item() for p in self.prm.parameters() if p.grad is not None),\n",
    "                    \"pred_mean\": pred.mean().item(),\n",
    "                    \"pred_std\": pred.std().item(),\n",
    "                    \"reward_mean\": reward.mean().item(),\n",
    "                    \"reward_std\": reward.std().item(),\n",
    "                })\n",
    "\n",
    "        return total / len(loader)\n",
    "\n",
    "    # ----------------------------------------------------------------- public\n",
    "    def fit(self, train_loader, val_loader) -> Dict[str, List[float]]:\n",
    "        self.cfg.dataset_size = len(train_loader) \n",
    "\n",
    "        history = {\"train\": [], \"val\": []}\n",
    "        best_val, bad_epochs, patience = float(\"inf\"), 0, 8  # patience 증가\n",
    "\n",
    "        for ep in range(self.cfg.epochs):\n",
    "            tr_loss = self._run_epoch(train_loader, train=True,  epoch_idx=ep)\n",
    "            vl_loss = self._run_epoch(val_loader,   train=False, epoch_idx=ep)\n",
    "\n",
    "            history[\"train\"].append(tr_loss)\n",
    "            history[\"val\"].append(vl_loss)\n",
    "            print(f\"[Epoch {ep+1}/{self.cfg.epochs}] train={tr_loss:.4f}  val={vl_loss:.4f}\")\n",
    "\n",
    "            # -------- epoch logging --------\n",
    "            if self.wandb_run:\n",
    "                wandb.log({\n",
    "                    \"train_loss\": tr_loss,\n",
    "                    \"val_loss\": vl_loss,\n",
    "                    \"epoch\": ep,\n",
    "                    \"lr\": self.opt.param_groups[0][\"lr\"],\n",
    "                })\n",
    "\n",
    "            # 체크포인트 저장\n",
    "            if vl_loss < best_val:\n",
    "                best_val = vl_loss\n",
    "                bad_epochs = 0\n",
    "                self._save_checkpoint(\"best_prm.pt\", epoch=ep, val_loss=vl_loss)\n",
    "                print(f\"[Best] New best validation loss: {vl_loss:.4f}\")\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "                print(f\"[Early-Stopping] No improvement for {bad_epochs}/{patience} epochs\")\n",
    "                if bad_epochs >= patience:\n",
    "                    print(f\"[Early-Stopping] Stopping training after {patience} epochs without improvement\")\n",
    "                    break\n",
    "        \n",
    "        self._save_checkpoint(\"last_prm.pt\", epoch=self.cfg.epochs - 1, val_loss=vl_loss)\n",
    "        return history\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # Checkpoint helpers\n",
    "    def _save_checkpoint(self, filename: str, *, epoch: int, val_loss: float) -> None:\n",
    "        path = self.ckpt_dir / filename\n",
    "        save_dict = {\n",
    "            \"epoch\": epoch,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"prm_state\": self.prm.state_dict(),\n",
    "            \"scheduler_state\": (self.scheduler.state_dict() if self.scheduler else None),\n",
    "            \"optimizer_state\": self.opt.state_dict(),\n",
    "            \"config\": vars(self.cfg),              # hyper‑params for reproducibility\n",
    "            \"model_name_or_path\": getattr(self.model, \"name_or_path\", None),\n",
    "            \"tokenizer_config\": self.tokenizer.__dict__.get(\"init_kwargs\", {}),\n",
    "        }\n",
    "        torch.save(save_dict, path)\n",
    "        print(f\"[CKPT] Saved ⇒ {path}\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Simple inference helper\n",
    "    @torch.no_grad()\n",
    "    def predict_reward(self, text: str) -> float:\n",
    "        ids = self.tokenizer.encode(text, return_tensors=\"pt\").to(self.device)\n",
    "        feat = self._encode(ids)\n",
    "        return float(torch.sigmoid(self.prm(feat)).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd412b",
   "metadata": {},
   "source": [
    "# Utils for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fafcc585",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_PATTERN = re.compile(\n",
    "    r\"\"\"^[\\s>#*\\-]*          # optional markdown/bullet symbols\n",
    "        Answer               # word 'Answer'\n",
    "        \\s*[:.\\-]\\s*         # separator\n",
    "        (.+?)\\s*$            # capture everything after\n",
    "    \"\"\",\n",
    "    re.IGNORECASE | re.MULTILINE | re.VERBOSE,\n",
    ")\n",
    "STEP_PATTERN = re.compile(r\"Step\\s*\\d+\\s*:\\s*(.*?)(?=\\nStep|\\nAnswer|$)\", re.S)\n",
    "# ANSWER_PATTERN = re.compile(r\"Answer\\s*:\\s*(.+?)\\s*$\", re.S)\n",
    "\n",
    "def build_prompt(question: str) -> str:\n",
    "    \"\"\"Return the prompt given a raw *question* string.\"\"\"\n",
    "    return f\"\"\"<|im_start|>system\n",
    "You are a helpful math tutor. You must solve problems step-by-step using the exact format:\n",
    "Step 1: [first step]\n",
    "Step 2: [second step]\n",
    "...\n",
    "Answer: [final answer]\n",
    "\n",
    "Example:\n",
    "Problem: What is 5 + 3?\n",
    "Step 1: Add 5 and 3\n",
    "Step 2: 5 + 3 = 8\n",
    "Answer: 8\n",
    "\n",
    "Now solve the given problem using the same format.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "def post_process_response(text: str) -> str:\n",
    "    # Step 패턴 찾기\n",
    "    step_pattern = r'Step\\s*\\d+:\\s*[^\\n]*'\n",
    "    steps = re.findall(step_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    # Answer 패턴 찾기\n",
    "    answer_pattern = r'Answer:\\s*([^\\n]*)'\n",
    "    answer_match = re.search(answer_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    if steps and answer_match:\n",
    "        # 올바른 형식으로 재구성\n",
    "        result = \"\\n\".join(steps)\n",
    "        result += f\"\\nAnswer: {answer_match.group(1).strip()}\"\n",
    "        return result\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def parse_steps_and_answer(text: str) -> Tuple[List[str], str]:\n",
    "    \"\"\"Extract step list and answer string from a generated trajectory.\"\"\"\n",
    "    steps = [m.group(1).strip() for m in STEP_PATTERN.finditer(text)]\n",
    "    ans_match = ANSWER_PATTERN.search(text)\n",
    "    answer = ans_match.group(1).strip() if ans_match else \"\"\n",
    "    return steps, answer\n",
    "\n",
    "def generate_candidates(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    prompt: str,\n",
    "    num_candidates: int,\n",
    "    gen_cfg: GenerationConfig,\n",
    "    device: torch.device,\n",
    ") -> List[str]:\n",
    "    \"\"\"Generate *num_candidates* reasoning trajectories for the prompt.\"\"\"\n",
    "    inputs = tokenizer([prompt] * num_candidates, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_cfg.to_dict())\n",
    "    texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    texts = [t[len(prompt):] for t in texts]\n",
    "    \n",
    "    # processed_texts = []\n",
    "    # for text in texts:\n",
    "    #     processed_text = post_process_response(text)\n",
    "    #     processed_texts.append(processed_text)\n",
    "    return texts\n",
    "\n",
    "def compute_step_rewards(\n",
    "    baseline: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    prm: ProcessRewardModel,\n",
    "    prm_device: torch.device,\n",
    "    prompt: str,\n",
    "    steps: List[str],\n",
    ") -> List[float]:\n",
    "    \"\"\"Return a list of scalar rewards (float) for each *completed* step.\"\"\"\n",
    "    rewards: List[float] = []\n",
    "\n",
    "    # We will iteratively feed *prompt + completed steps* through baseline.\n",
    "    cumulative_text = prompt\n",
    "    for i, step_txt in enumerate(steps):\n",
    "        cumulative_text += f\"Step {i + 1}: {step_txt}\\n\"\n",
    "        tokens = tokenizer(cumulative_text, return_tensors=\"pt\").to(prm_device)\n",
    "        with torch.no_grad():\n",
    "            outputs = baseline(**tokens, output_hidden_states=True)\n",
    "        # Use hidden states of the last token (or pool as needed)\n",
    "        last_hidden = outputs.hidden_states[-1][0, -1, :]  # (hidden_dim,)\n",
    "        last_hidden = last_hidden.float() \n",
    "        reward = prm(last_hidden.unsqueeze(0)).item()  # type: ignore\n",
    "        rewards.append(reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d096bb",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e566a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Loading Baseline andPRM!\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Load baseline LM -------------------\n",
    "model_name = \"Qwen/Qwen2.5-Math-7B-Instruct\" \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "baseline = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# ------------------- Load PRM ---------------------------\n",
    "prm_ckpt_path = \"/home/leena/ccc_eval/mcts_prm/prm_dataset2/checkpoints/0715/contri/best_prm.pt\"\n",
    "prm_ckpt = torch.load(prm_ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "prm_cfg = PRMConfig(**prm_ckpt.get(\"cfg\", {}))\n",
    "prm = ProcessRewardModel(baseline.config.hidden_size, cfg=prm_cfg)\n",
    "prm.load_state_dict(prm_ckpt[\"prm_state\"])\n",
    "prm = prm.float()  # 명시적으로 Float32로 설정\n",
    "prm = prm.to(device).eval()\n",
    "print(\"Finish Loading Baseline and PRM!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a98f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Dataset ---------------------------\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "max_samples = 2\n",
    "if max_samples:\n",
    "    ds = ds.select(range(max_samples))\n",
    "loader = DataLoader(ds, batch_size=1, shuffle=False)\n",
    "print(\"Finish Loading Dataset!\")\n",
    "\n",
    "config = PRMConfig()\n",
    "gen_cfg = GenerationConfig(\n",
    "    temperature=0.3,\n",
    "    top_p=0.8,\n",
    "    max_new_tokens=config.max_new_tokens,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id= tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    ")\n",
    "num_candidates = 6\n",
    "results = []\n",
    "for idx, sample in tqdm(enumerate(loader)):\n",
    "    question = sample[\"question\"][0]\n",
    "    gold = sample.get(\"answer\", [\"\"])[0]\n",
    "    prompt = build_prompt(question)\n",
    "\n",
    "    # 1) Generate candidate CoTs\n",
    "    cand_texts = generate_candidates(\n",
    "        baseline,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        num_candidates,\n",
    "        gen_cfg,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    # 2) Score each candidate via PRM\n",
    "    cand_scores: List[float] = []\n",
    "    cand_answers: List[str] = []\n",
    "    best_chain = \"\"\n",
    "    for text in cand_texts:\n",
    "        steps, answer = parse_steps_and_answer(text)\n",
    "        print(\"Step/Answer:\",steps, answer)\n",
    "        step_rewards = compute_step_rewards(baseline, tokenizer, prm, device, prompt, steps)\n",
    "        print(\"step rewards:\",step_rewards)\n",
    "        total_r = sum(step_rewards)\n",
    "        cand_scores.append(total_r)\n",
    "        cand_answers.append(answer)\n",
    "        # Keep full chain for printing if it wins\n",
    "        if total_r == max(cand_scores):\n",
    "            best_chain = text\n",
    "\n",
    "    best_idx = int(torch.tensor(cand_scores).argmax().item())\n",
    "    best_answer = cand_answers[best_idx]\n",
    "    best_score = cand_scores[best_idx]\n",
    "\n",
    "    # 3) Save result\n",
    "    results.append(\n",
    "        {\n",
    "            \"id\": sample.get(\"id\", [idx])[0] if isinstance(sample.get(\"id\", [idx]), list) else idx,\n",
    "            \"question\": question,\n",
    "            \"gold\": gold,\n",
    "            \"pred\": best_answer,\n",
    "            \"chain\": best_chain,\n",
    "            \"score\": best_score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if (idx + 1) % 20 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(loader)} samples…\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d7319",
   "metadata": {},
   "source": [
    "# Merge Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb3cfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960\n",
      "3152\n",
      "4112\n",
      "4112\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/home/leena/ccc_eval/mcts_prm/cmi_samples/math_contri_mistral_0_960.json\", \"r\") as file:\n",
    "    f1 = json.load(file)\n",
    "\n",
    "with open(\"/home/leena/ccc_eval/mcts_prm/cmi_samples/math_contri_mistral_960_3153.json\", \"r\") as file:\n",
    "    f2 = json.load(file)\n",
    "\n",
    "print(len(f1))\n",
    "print(len(f2))    \n",
    "\n",
    "merged_data = f1 + f2\n",
    "print(len(merged_data))\n",
    "\n",
    "with open(\"/home/leena/ccc_eval/mcts_prm/cmi_samples/math_contri_mistral_0_3153.json\", \"w\") as f:\n",
    "    json.dump(merged_data, f, indent=2)\n",
    "\n",
    "with open(\"/home/leena/ccc_eval/mcts_prm/cmi_samples/math_contri_mistral_0_3153.json\", \"r\") as file:\n",
    "    saved = json.load(file)\n",
    "\n",
    "print(len(saved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61a1f6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'What is the domain of the function $f(x)=\\\\log_2(\\\\log_3(\\\\log_4(\\\\log_5x)))$?',\n",
       " 'level': 'Level 4',\n",
       " 'type': 'Intermediate Algebra',\n",
       " 'solution': 'In order for the given function to have a real value, $\\\\log_3(\\\\log_4(\\\\log_5x))>0$ (since the logarithm of only any positive number is real). In order for the last inequality to be true, $\\\\log_4(\\\\log_5x)>1$ (since the logarithm of only any number greater than 1 is greater than 0). The last inequality is true only if $\\\\log_5x>4^1=4$, so $x>5^4\\\\Rightarrow x>625,$ or in interval notation, $x \\\\in \\\\boxed{(625, \\\\infty)}.$'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"HuggingFaceTB/MATH\", \"all\", split=\"train\")\n",
    "ds[4112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28fa92ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted /home/leena/ccc_eval/mcts_prm/cmi_samples/math_contri_mistral_4112_4681.jsonl to /home/leena/ccc_eval/mcts_prm/cmi_samples/math_contri_mistral_4112_4681.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def jsonl_to_json(jsonl_path, json_path):\n",
    "    data = read_jsonl(jsonl_path)\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Converted {jsonl_path} to {json_path}\")\n",
    "\n",
    "jsonl_file = \"/home/leena/ccc_eval/mcts_prm/cmi_samples/math_contri_mistral_4112_4681.jsonl\"\n",
    "json_file = \"/home/leena/ccc_eval/mcts_prm/cmi_samples/math_contri_mistral_4112_4681.json\"\n",
    "jsonl_to_json(jsonl_file, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 경로 설정\n",
    "contri_file = \"/home/leena/ccc_eval/mcts_prm/cmi_samples/math_contri_mistral_total.json\"\n",
    "mi_file = \"/home/leena/ccc_eval/mcts_prm/cmi_samples/math_mi_mistral_total.json\"\n",
    "output_file = \"/home/leena/ccc_eval/mcts_prm/cmi_samples/total_math_merge_mistral.json\"\n",
    "\n",
    "with open(contri_file, \"r\") as file:\n",
    "    contri_data = json.load(file)\n",
    "\n",
    "with open(mi_file, \"r\") as file:\n",
    "    mi_data = json.load(file)\n",
    "\n",
    "\n",
    "def make_entry_key(entry: dict) -> tuple:\n",
    "    return (\n",
    "        entry[\"question\"].strip(),\n",
    "        tuple(entry[\"completion\"]),  # 리스트를 튜플로 변환하여 해시 가능하게\n",
    "        entry[\"gold_answer\"].strip()\n",
    "    )\n",
    "\n",
    "merged_dict = {}\n",
    "print(\"Processing contribution data...\")\n",
    "for entry in contri_data:\n",
    "    try:\n",
    "        key = make_entry_key(entry)\n",
    "        merged_dict[key] = entry.copy()\n",
    "    except KeyError as e:\n",
    "        print(f\"Warning: Entry missing required field: {e}\")\n",
    "        continue\n",
    "print(\"Merging mutual information data...\")\n",
    "merged_count = 0\n",
    "new_count = 0\n",
    "error_count = 0\n",
    "for entry in mi_data:\n",
    "    try:\n",
    "        key = make_entry_key(entry)\n",
    "        if key in merged_dict:\n",
    "            merged_dict[key].update({\n",
    "                \"mi_rewards\": entry.get(\"mi_rewards\"),\n",
    "                \"mi_filtered\": entry.get(\"mi_filtered\")\n",
    "            })\n",
    "            merged_count += 1\n",
    "        else:\n",
    "            merged_dict[key] = entry.copy()\n",
    "            new_count += 1\n",
    "    except KeyError as e:\n",
    "        print(f\"Warning: Entry missing required field: {e}\")\n",
    "        error_count += 1\n",
    "        continue\n",
    "\n",
    "merged_data = list(merged_dict.values())\n",
    "\n",
    "print(f\"\\nMerge Summary:\")\n",
    "print(f\"  Contribution entries: {len(contri_data)}\")\n",
    "print(f\"  MI entries: {len(mi_data)}\")\n",
    "print(f\"  Merged entries: {len(merged_data)}\")\n",
    "print(f\"  Successfully merged: {merged_count}\")\n",
    "print(f\"  New entries added: {new_count}\")\n",
    "print(f\"  Errors encountered: {error_count}\")\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "print(\"✅ Merge completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load(path: str):\n",
    "    \"\"\"json 파일을 읽어 list[dict] 반환.\"\"\"\n",
    "    return json.loads(Path(path).read_text(encoding=\"utf‑8\"))\n",
    "\n",
    "def save(obj, path: str):\n",
    "    Path(path).write_text(\n",
    "        json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf‑8\"\n",
    "    )\n",
    "\n",
    "def make_key(entry: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    중복 여부를 판정할 key 생성.\n",
    "    - question 은 공백 차이를 없애기 위해 strip\n",
    "    - completion 은 순서를 보존하기 위해 튜플로 변환\n",
    "    - gold_answer 도 strip\n",
    "    \"\"\"\n",
    "    return (\n",
    "        entry[\"question\"].strip(),\n",
    "        tuple(entry[\"completion\"]),\n",
    "        entry[\"gold_answer\"].strip(),\n",
    "    )\n",
    "\n",
    "def merge_files(file1: str, file2: str, out: str = \"merged.json\"):\n",
    "    data1, data2 = load(file1), load(file2)\n",
    "\n",
    "    # 1️⃣ file1 을 기준으로 dict 초기화 (mi_* 포함)\n",
    "    merged = {make_key(e): e.copy() for e in data1}\n",
    "\n",
    "    # 2️⃣ file2 의 ori/ptb/contributions 를 덧붙임\n",
    "    for e in data2:\n",
    "        k = make_key(e)\n",
    "        if k not in merged:          # 혹시 file1 에 없으면 그대로 추가\n",
    "            merged[k] = e.copy()\n",
    "        else:                        # 이미 있으면 reward 값만 update\n",
    "            merged[k].update(\n",
    "                {\n",
    "                    \"ori_rewards\":      e.get(\"ori_rewards\"),\n",
    "                    \"ptb_rewards\":      e.get(\"ptb_rewards\"),\n",
    "                    \"contributions\":    e.get(\"contributions\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # 3️⃣ list 로 변환 후 저장\n",
    "    save(list(merged.values()), out)\n",
    "    print(f\"✅  merged {len(merged)} entries → {out}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_files(\"file1.json\", \"file2.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
