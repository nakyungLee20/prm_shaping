{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, PreTrainedTokenizer\n",
    "from collections import defaultdict, deque\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm, trange\n",
    "import wandb\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from collections import Counter\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.40it/s]\n",
      "Some weights of the model checkpoint at Qwen/Qwen2.5-Math-PRM-7B were not used when initializing Qwen2ForProcessRewardModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Math-PRM-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, device_map=\"auto\",  torch_dtype=torch.bfloat16, trust_remote_code=True,).eval()\n",
    "\n",
    "data = {\n",
    "    \"system\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\",\n",
    "    \"query\": \"Sue lives in a fun neighborhood.  One weekend, the neighbors decided to play a prank on Sue. On Friday morning, the neighbors placed 18 pink plastic flamingos out on Sue's front yard. On Saturday morning, the neighbors took back one third of the flamingos, painted them white, and put these newly painted white flamingos back out on Sue's front yard.  Then, on Sunday morning, they added another 18 pink plastic flamingos to the collection. At noon on Sunday, how many more pink plastic flamingos were out than white plastic flamingos?\",\n",
    "    \"response\": [\n",
    "      \"To find out how many more pink plastic flamingos were out than white plastic flamingos at noon on Sunday, we can break down the problem into steps. First, on Friday, the neighbors start with 18 pink plastic flamingos.\",\n",
    "      \"On Saturday, they take back one third of the flamingos. Since there were 18 flamingos, (1/3 \\\\times 18 = 6) flamingos are taken back. So, they have (18 - 6 = 12) flamingos left in their possession. Then, they paint these 6 flamingos white and put them back out on Sue's front yard. Now, Sue has the original 12 pink flamingos plus the 6 new white ones. Thus, by the end of Saturday, Sue has (12 + 6 = 18) pink flamingos and 6 white flamingos.\",\n",
    "      \"On Sunday, the neighbors add another 18 pink plastic flamingos to Sue's front yard. By the end of Sunday morning, Sue has (18 + 18 = 36) pink flamingos and still 6 white flamingos.\",\n",
    "      \"To find the difference, subtract the number of white flamingos from the number of pink flamingos: (36 - 6 = 30). Therefore, at noon on Sunday, there were 30 more pink plastic flamingos out than white plastic flamingos. The answer is (\\\\boxed{30}).\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_step_rewards(logits, token_masks):\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "    probabilities = probabilities * token_masks.unsqueeze(-1) # bs, seq_len, num_labels\n",
    "\n",
    "    all_scores_res = []\n",
    "    for i in range(probabilities.size(0)):\n",
    "        sample = probabilities[i] # seq_len, num_labels [452, 2]\n",
    "        positive_probs = sample[sample != 0].view(-1, 2)[:, 1] # valid_tokens, num_labels [# of steps, label]\n",
    "        non_zero_elements_list = positive_probs.cpu().tolist()\n",
    "        all_scores_res.append(non_zero_elements_list)\n",
    "    return all_scores_res\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": data['system']},\n",
    "    {\"role\": \"user\", \"content\": data['query']},\n",
    "    {\"role\": \"assistant\", \"content\": \"<extra_0>\".join(data['response']) + \"<extra_0>\"},\n",
    "]\n",
    "conversation_str = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=False\n",
    ")\n",
    "\n",
    "input_ids = tokenizer.encode(conversation_str, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model(input_ids=input_ids)\n",
    "\n",
    "step_sep_id = tokenizer.encode(\"<extra_0>\")[0]\n",
    "token_masks = (input_ids == step_sep_id)\n",
    "step_reward = make_step_rewards(outputs[0], token_masks)\n",
    "print(step_reward)  # [[1.0, 0.1904296875, 0.9765625, 1.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token '<extra_0>': ID 151651\n",
      "Token '<extra_0>': Error - Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
      "Token '<extra_1>': ID 27\n",
      "Token '<extra_1>': Error - Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
      "Token '<extra_2>': ID 27\n",
      "Token '<extra_2>': Error - Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
      "Token '|STEP|': ID 91\n",
      "Token '|STEP|': Error - Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n"
     ]
    }
   ],
   "source": [
    "def check_token_learning():\n",
    "    test_tokens = [\"<extra_0>\", \"<extra_1>\", \"<extra_2>\", \"|STEP|\"]\n",
    "    for token in test_tokens:\n",
    "        try:\n",
    "            token_id = tokenizer.encode(token)[0]\n",
    "            print(f\"Token '{token}': ID {token_id}\")\n",
    "            if hasattr(model, 'get_input_embeddings'):\n",
    "                embedding = model.get_input_embeddings()\n",
    "                token_embedding = embedding(torch.tensor([token_id]))\n",
    "                print(f\"  Embedding shape: {token_embedding.shape}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Token '{token}': Error - {e}\")\n",
    "\n",
    "check_token_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorrect Step Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import copy\n",
    "\n",
    "def create_incorrect_steps(gold_steps: List[str], incorrect_type: str = \"wrong_calculation\") -> Tuple[List[str], int]:\n",
    "    if not gold_steps:\n",
    "        return gold_steps\n",
    "\n",
    "    perturbed_steps = gold_steps.copy()\n",
    "    insert_position = random.randint(1, len(gold_steps))   # 1 ≤ pos ≤ len\n",
    "    \n",
    "    if incorrect_type == \"wrong_calculation\":\n",
    "        wrong_calculations = [\n",
    "            f\"Step {insert_position + 1}: 5 + 3 = 9\",  # 5 + 3 = 8\n",
    "            f\"Step {insert_position + 1}: 10 * 2 = 15\",  # 10 * 2 = 20\n",
    "            f\"Step {insert_position + 1}: 20 / 4 = 6\",  # 20 / 4 = 5\n",
    "            f\"Step {insert_position + 1}: 7 - 3 = 5\",  # 7 - 3 = 4\n",
    "            f\"Step {insert_position + 1}: 2^2 = 5\",  # 2^2 = 4\n",
    "            f\"Step {insert_position + 1}: sqrt(16) = 3\",  # sqrt(16) = 4\n",
    "            f\"Step {insert_position + 1}: 3 * 7 = 24\",  # 3 * 7 = 21\n",
    "            f\"Step {insert_position + 1}: 15 / 3 = 6\",  # 15 / 3 = 5\n",
    "        ]\n",
    "        insert_step = random.choice(wrong_calculations)\n",
    "    elif incorrect_type == \"logical_error\":\n",
    "        logical_errors = [\n",
    "            f\"Step {insert_position + 1}: Since we need to find the total, we should multiply by 0 instead of adding.\",\n",
    "            f\"Step {insert_position + 1}: To solve this, we need to subtract the larger number from the smaller one.\",\n",
    "            f\"Step {insert_position + 1}: The answer should be negative because we're dealing with positive numbers.\",\n",
    "            f\"Step {insert_position + 1}: We can ignore the units since they don't affect the calculation.\",\n",
    "            f\"Step {insert_position + 1}: The order of operations doesn't matter here, so we can do addition first.\",\n",
    "        ]\n",
    "        insert_step = random.choice(logical_errors)\n",
    "    elif incorrect_type == \"irrelevant\":\n",
    "        irrelevant_steps = [\n",
    "            f\"Step {insert_position + 1}: The weather is nice today.\",\n",
    "            f\"Step {insert_position + 1}: I like literatures very much.\",\n",
    "            f\"Step {insert_position + 1}: This reminds me of my school days.\",\n",
    "            f\"Step {insert_position + 1}: The sky is blue and beautiful.\",\n",
    "            f\"Step {insert_position + 1}: I should drink more water.\",\n",
    "            f\"Step {insert_position + 1}: Python is the language of the universe.\",\n",
    "        ]\n",
    "        insert_step = random.choice(irrelevant_steps)\n",
    "    elif incorrect_type == \"repetition\":\n",
    "        if len(gold_steps) > 1:\n",
    "            repeat_step = gold_steps[insert_position - 1]  # 이전 step\n",
    "            step_num = insert_position + 1\n",
    "            step_content = repeat_step.split(\":\", 1)[1] if \":\" in repeat_step else \"\"\n",
    "            insert_step = f\"Step {step_num}:{step_content}\"\n",
    "        else:\n",
    "            insert_position = 1\n",
    "            step_num = insert_position + 1\n",
    "            repeat_step = gold_steps[0]\n",
    "            step_content = repeat_step.split(\":\", 1)[1] if \":\" in repeat_step else \"\"\n",
    "            insert_step = f\"Step {step_num}:{step_content}\"\n",
    "    else:\n",
    "        insert_step = f\"Step {insert_position + 1}: 5 + 3 = 9\"\n",
    "    \n",
    "    perturbed_steps.insert(insert_position, insert_step)\n",
    "\n",
    "    for i in range(insert_position + 1, len(perturbed_steps)):\n",
    "            if perturbed_steps[i].startswith(\"Step \"):\n",
    "                step_num = i + 1\n",
    "                step_content = perturbed_steps[i].split(\":\", 1)[1] if \":\" in perturbed_steps[i] else \"\"\n",
    "                perturbed_steps[i] = f\"Step {step_num}:{step_content}\"\n",
    "    \n",
    "    return perturbed_steps, insert_position\n",
    "\n",
    "def add_incorrect_completion(entry: Dict[str, Any], incorrect_type: str = \"wrong_calculation\", negative_reward: float = -1.0) -> Dict[str, Any]:\n",
    "    new_entry = copy.deepcopy(entry)\n",
    "    original_completion = entry.get(\"completion\", [])\n",
    "    # incorrect completion 생성\n",
    "    incorrect_completion, insert_position = create_incorrect_steps(original_completion, incorrect_type)\n",
    "    new_entry[\"completion\"] = incorrect_completion\n",
    "    # incorrect rewards 생성\n",
    "    for key, val in entry.items():\n",
    "        if isinstance(val, list) and len(val) == len(original_completion) and key != \"completion\":\n",
    "            new_vec = val.copy()\n",
    "            if key == \"contributions\":\n",
    "                new_vec.insert(insert_position, negative_reward)\n",
    "            elif key == \"mi_filtered\":\n",
    "                new_vec.insert(insert_position, 0.0)\n",
    "            else:\n",
    "                new_vec.insert(insert_position, negative_reward)\n",
    "            new_entry[key] = new_vec\n",
    "    # meta data 추가\n",
    "    new_entry[\"is_incorrect\"] = True\n",
    "    new_entry[\"incorrect_type\"] = incorrect_type\n",
    "\n",
    "    return new_entry\n",
    "\n",
    "def extend_dataset_with_incorrect_steps(dataset: List[Dict[str, Any]], incorrect_types: List[str] = None,negative_rewards: List[float] = None, ratio: float = 0.5) -> List[Dict[str, Any]]:\n",
    "    if incorrect_types is None:\n",
    "        incorrect_types = [\"wrong_calculation\", \"logical_error\", \"irrelevant_step\", \"repetition\"]\n",
    "    if negative_rewards is None:\n",
    "        negative_rewards = [-1.0] * len(incorrect_types)\n",
    "    \n",
    "    # negative_rewards를 incorrect_types와 매칭\n",
    "    if len(negative_rewards) != len(incorrect_types):\n",
    "        negative_rewards = negative_rewards * (len(incorrect_types) // len(negative_rewards) + 1)\n",
    "        negative_rewards = negative_rewards[:len(incorrect_types)]\n",
    "    \n",
    "    extended_dataset = []\n",
    "    for entry in dataset:\n",
    "        extended_dataset.append(entry)\n",
    "        if random.random() < ratio:\n",
    "            incorrect_type = random.choice(incorrect_types) # 랜덤하게 incorrect type 선택\n",
    "            type_idx = incorrect_types.index(incorrect_type)\n",
    "            negative_reward = negative_rewards[type_idx]\n",
    "            incorrect_entry = add_incorrect_completion(entry, incorrect_type, negative_reward)\n",
    "            extended_dataset.append(incorrect_entry)\n",
    "    \n",
    "    return extended_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 7473\n",
      "Extended dataset size: 11141\n",
      "Added 3668 incorrect samples\n"
     ]
    }
   ],
   "source": [
    "input_path = \"/home/leena/ccc_eval/mcts_prm/cmi_samples/total_gsm8k_merge_mistral.json\"\n",
    "output_path = \"/home/leena/ccc_eval/mcts_prm/cmi_samples/total_gsm8k_merge_mistral_incorrect.json\"\n",
    "\n",
    "with open(input_path, \"r\") as file:\n",
    "    dataset = json.load(file)\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "\n",
    "extended_dataset = extend_dataset_with_incorrect_steps(dataset=dataset, ratio=0.5)\n",
    "print(f\"Extended dataset size: {len(extended_dataset)}\")\n",
    "print(f\"Added {len(extended_dataset) - len(dataset)} incorrect samples\")\n",
    "\n",
    "# 확장된 dataset 저장\n",
    "with open(output_path, \"w\") as file2:\n",
    "    json.dump(extended_dataset, file2, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct samples: 7473\n",
      "Incorrect samples: 3668\n",
      "\n",
      "Incorrect types distribution:\n",
      "  repetition: 911\n",
      "  wrong_calculation: 928\n",
      "  irrelevant_step: 923\n",
      "  logical_error: 906\n"
     ]
    }
   ],
   "source": [
    "# 통계 출력\n",
    "correct_count = sum(1 for entry in extended_dataset if not entry.get(\"is_incorrect\", False))\n",
    "incorrect_count = sum(1 for entry in extended_dataset if entry.get(\"is_incorrect\", False))\n",
    "\n",
    "print(f\"Correct samples: {correct_count}\")\n",
    "print(f\"Incorrect samples: {incorrect_count}\")\n",
    "\n",
    "# 타입별 통계\n",
    "type_counts = {}\n",
    "for entry in extended_dataset:\n",
    "    if entry.get(\"is_incorrect\", False):\n",
    "        incorrect_type = entry.get(\"incorrect_type\", \"unknown\")\n",
    "        type_counts[incorrect_type] = type_counts.get(incorrect_type, 0) + 1\n",
    "\n",
    "print(\"\\nIncorrect types distribution:\")\n",
    "for incorrect_type, count in type_counts.items():\n",
    "    print(f\"  {incorrect_type}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add incorrect MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성공: '/home/leena/ccc_eval/mcts_prm/cmi_samples/math_mi_mistral_full.jsonl' 파일이 '/home/leena/ccc_eval/mcts_prm/cmi_samples/test_json.json' 파일로 성공적으로 변환되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def convert_jsonl_to_json(jsonl_file_path, json_file_path):\n",
    "    json_objects = []\n",
    "    \n",
    "    try:\n",
    "        with open(jsonl_file_path, 'r', encoding='utf-8') as infile:\n",
    "            for line in infile:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    json_objects.append(json.loads(line))\n",
    "        \n",
    "        with open(json_file_path, 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(json_objects, outfile, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        print(f\"성공: '{jsonl_file_path}' 파일이 '{json_file_path}' 파일로 성공적으로 변환되었습니다.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: 입력 파일 '{jsonl_file_path}'을(를) 찾을 수 없습니다.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"오류: '{jsonl_file_path}' 파일 처리 중 JSON 파싱 오류가 발생했습니다. 오류 내용: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"알 수 없는 오류가 발생했습니다: {e}\")\n",
    "\n",
    "input_file_name = '/home/leena/ccc_eval/mcts_prm/cmi_samples/math_mi_mistral_full.jsonl'\n",
    "output_file_name = '/home/leena/ccc_eval/mcts_prm/cmi_samples/test_json.json'\n",
    "convert_jsonl_to_json(input_file_name, output_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, copy, random, re\n",
    "from typing import List, Dict, Any, Optional\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# ======================= Expanded Banks =======================\n",
    "SELF_REFLECTION_BANK = [\n",
    "    \"Let me think about this problem carefully.\",\n",
    "    \"I need to check my calculations.\",\n",
    "    \"This step seems important for the solution.\",\n",
    "    \"Let me verify the previous step.\",\n",
    "    \"I should double-check my work.\",\n",
    "    \"This is a crucial part of the solution.\",\n",
    "    \"Let me organize my thoughts.\",\n",
    "    \"I need to be careful with the math.\",\n",
    "    \"I should confirm the units are consistent.\",\n",
    "    \"Let me restate the given conditions precisely.\",\n",
    "    \"I might have made an algebraic slip; let me re-derive.\",\n",
    "    \"I should check edge cases and constraints.\",\n",
    "    \"Let me simplify the expression before substituting values.\",\n",
    "    \"I should verify if I used the correct formula.\",\n",
    "    \"Let me check whether I applied the operation in the right order.\",\n",
    "    \"I should compute a quick sanity check with approximations.\",\n",
    "    \"Let me recompute using an alternative method to confirm.\",\n",
    "    \"I should cross-check with the final requirement of the question.\",\n",
    "    \"Let me verify that each transformation is logically valid.\",\n",
    "    \"I should test with a small example to validate the pattern.\",\n",
    "]\n",
    "\n",
    "WRONG_STEP_BANK = [\n",
    "    # 산술/대수\n",
    "    \"5 + 3 = 9\",\n",
    "    \"10 * 2 = 15\",\n",
    "    \"20 / 4 = 6\",\n",
    "    \"x/2 = 6, so x = 10\",\n",
    "    \"2^2 = 5\",\n",
    "    \"\\\\sqrt(36) = 5\",\n",
    "    \"Perimeter of square side 5 = 15\",\n",
    "    # 추가: 분수/부호/분배/지수/로그/근사\n",
    "    \"1/3 + 1/6 = 1/9\",\n",
    "    \"(-2)^2 = -4\",\n",
    "    \"2(x + 3) = 2x + 3\",\n",
    "    \"x^a * x^b = x^{a-b}\",\n",
    "    \"\\\\log(ab) = \\\\log a - \\\\log b\",\n",
    "    \"\\\\sqrt{a+b} = \\\\sqrt a + \\\\sqrt b\",\n",
    "    \"1/0 = 0\",\n",
    "    \"0^0 = 0\",\n",
    "    \"7/10 ≈ 0.9\",\n",
    "    # 기하\n",
    "    \"Area of triangle = base + height\",\n",
    "    \"Circumference of a circle with r=3 is 3r\",\n",
    "    \"Area of a circle with r=3 is 2\\\\pi r^2\",\n",
    "    \"Pythagorean theorem: a + b = c\",\n",
    "    # 확률/통계\n",
    "    \"P(A \\\\cap B) = P(A) + P(B)\",\n",
    "    \"Variance of cX is Var(X) + c\",\n",
    "    \"Mean of [2,4,9] is 6\",\n",
    "    # 미적분\n",
    "    \"d/dx (x^2) = 2\",\n",
    "    \"∫ x dx = x^2 + C (missing 1/2)\",\n",
    "    \"Derivative of sin x is -cos x\",\n",
    "    \"Product rule: (fg)' = f' + g'\",\n",
    "    # 방정식 처리\n",
    "    \"From 2x = 6, x = 2 (dividing by 2 and subtracting 2)\",\n",
    "    \"If x/y = 2/3, then x = y (cross-multiplication error)\",\n",
    "]\n",
    "\n",
    "IRRELEVANT_BANK = [\n",
    "    \"The weather is nice today.\",\n",
    "    \"I like mathematics very much.\",\n",
    "    \"This reminds me of my school days.\",\n",
    "    \"The sky is blue and beautiful.\",\n",
    "    \"I should drink more water.\",\n",
    "    \"Patterns exist in everything.\",\n",
    "    \"I should make a grocery list.\",\n",
    "    \"I wonder what to cook for dinner.\",\n",
    "    \"This pencil needs sharpening.\",\n",
    "    \"I should clean my desk later.\",\n",
    "    \"The soundtrack from that movie is stuck in my head.\",\n",
    "    \"My cat was very energetic this morning.\",\n",
    "    \"I might take a walk after finishing this.\",\n",
    "    \"I should reply to that email soon.\",\n",
    "    \"I forgot to water the plants yesterday.\",\n",
    "    \"I wonder if it's going to rain tomorrow.\",\n",
    "    \"I should back up my files.\",\n",
    "    \"This coffee tastes a bit strong.\",\n",
    "    \"I need to charge my phone.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renumber_steps(steps: List[str], start_idx: int = 0) -> List[str]:\n",
    "    new_steps: List[str] = []\n",
    "    for i in range(len(steps)):\n",
    "        s = steps[i]\n",
    "        content = s.split(\":\", 1)[1] if \":\" in s else s\n",
    "        new_steps.append(f\"Step {start_idx + i + 1}:{content}\")\n",
    "    return new_steps\n",
    "\n",
    "def _is_numeric_list(v: Any, expected_len: int) -> bool:\n",
    "    if not isinstance(v, list) or len(v) != expected_len:\n",
    "        return False\n",
    "    try:\n",
    "        _ = [float(x) for x in v]\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _sample_perturbation_type(perturbation_probs: Optional[Dict[str, float]] = None, rng: Optional[random.Random] = None,) -> str:\n",
    "    R = rng or random\n",
    "    types = [\"wrong_step\", \"irrelevant\", \"self_reflection\"]\n",
    "    if perturbation_probs:\n",
    "        # normalize\n",
    "        items = [(t, max(0.0, float(perturbation_probs.get(t, 0.0)))) for t in types]\n",
    "        total = sum(w for _, w in items)\n",
    "        if total <= 0:\n",
    "            weights = [1/3, 1/3, 1/3]\n",
    "        else:\n",
    "            weights = [w/total for _, w in items]\n",
    "    else:\n",
    "        weights = [1/3, 1/3, 1/3]\n",
    "    return random.choices(types, weights=weights, k=1)[0]\n",
    "\n",
    "def create_perturbed_steps(steps: List[str], typ: str, insert_pos: int, rng: Optional[random.Random] = None) -> (List[str], int):\n",
    "    R = rng or random\n",
    "    assert 0 <= insert_pos <= len(steps)\n",
    "    new_steps = steps.copy()\n",
    "    if typ == \"wrong_step\":\n",
    "        ins = f\"Step {insert_pos + 1}: {random.choice(WRONG_STEP_BANK)}\"\n",
    "    elif typ == \"irrelevant\":\n",
    "        ins = f\"Step {insert_pos + 1}: {random.choice(IRRELEVANT_BANK)}\"\n",
    "    else:\n",
    "        ins = f\"Step {insert_pos + 1}: {random.choice(SELF_REFLECTION_BANK)}\"\n",
    "    new_steps.insert(insert_pos, ins)\n",
    "    new_steps = renumber_steps(new_steps)\n",
    "    return new_steps, insert_pos\n",
    "\n",
    "def _robust_z(x: np.ndarray, clip_z: float = 3.0) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    med = np.median(x)\n",
    "    mad = np.median(np.abs(x - med)) + 1e-8\n",
    "    z = (x - med) / (1.4826 * mad)\n",
    "    if clip_z is not None:\n",
    "        z = np.clip(z, -clip_z, clip_z)\n",
    "    return z\n",
    "\n",
    "def _normalize_signed(x: np.ndarray, tau=1.5, clip_z=3.0, deadzone=0.2):\n",
    "    z = _robust_z(x, clip_z=clip_z)\n",
    "    s = np.tanh(z / max(tau, 1e-8))   # [-1,1]\n",
    "    if deadzone and deadzone > 0.0:\n",
    "        mag = np.maximum(0.0, np.abs(s) - deadzone) / (1.0 - deadzone)\n",
    "        s = np.sign(s) * mag\n",
    "    return s\n",
    "\n",
    "def recompute_mi_norm_with_ignore(raw: List[float], incorrect_mask: List[int],*, mode: str = \"unit\",         # \"signed\" | \"unit\" | \"minmax\" | \"relu\"\n",
    "    tau: float = 1.5, clip_z: float = 3.0, deadzone: float = 0.2, q_low: float = 5.0, q_high: float = 95.0, round_to: int = 4) -> List[float]:\n",
    "    x = np.asarray(raw, dtype=float)\n",
    "    mask = np.asarray(incorrect_mask, dtype=int)\n",
    "    keep = (mask == 0)\n",
    "\n",
    "    x_keep = x[keep] if np.any(keep) else x\n",
    "    if mode == \"relu\":\n",
    "        y = np.maximum(x_keep, 0.0)\n",
    "        out = np.zeros_like(x, dtype=float)\n",
    "        out[keep] = y\n",
    "        out[~keep] = 0.0\n",
    "    elif mode == \"signed\":\n",
    "        y = _normalize_signed(x_keep, tau=tau, clip_z=clip_z, deadzone=deadzone)\n",
    "        out = np.zeros_like(x, dtype=float)\n",
    "        out[keep] = y\n",
    "        out[~keep] = -1.0\n",
    "    elif mode == \"unit\":\n",
    "        y = _normalize_signed(x_keep, tau=tau, clip_z=clip_z, deadzone=deadzone)\n",
    "        y = 0.5 * (y + 1.0)\n",
    "        out = np.zeros_like(x, dtype=float)\n",
    "        out[keep] = y\n",
    "        out[~keep] = 0.0\n",
    "    elif mode == \"minmax\":\n",
    "        lo = np.percentile(x_keep, q_low) if len(x_keep) else np.min(x)\n",
    "        hi = np.percentile(x_keep, q_high) if len(x_keep) else np.max(x)\n",
    "        if hi <= lo:\n",
    "            scale = np.zeros_like(x_keep)\n",
    "        else:\n",
    "            scale = np.clip((x_keep - lo) / (hi - lo), 0.0, 1.0)\n",
    "        out = np.zeros_like(x, dtype=float)\n",
    "        out[keep] = scale\n",
    "        out[~keep] = 0.0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "    if round_to is not None:\n",
    "        out = np.round(out.astype(float), round_to)\n",
    "    return out.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_incorrect_step( entry: Dict[str, Any],*, min_raw_value: float = -1e6, norm_mode_for_mi_norm: str = \"signed\", norm_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    perturbation_probs: Optional[Dict[str, float]] = None, rng: Optional[random.Random] = None,) -> Dict[str, Any]:\n",
    "    \n",
    "    R = rng or random\n",
    "    e = copy.deepcopy(entry)\n",
    "    steps = e.get(\"completion\", [])\n",
    "    if not isinstance(steps, list) or len(steps) == 0:\n",
    "        return e\n",
    "\n",
    "    L = len(steps)\n",
    "    insert_pos = random.randint(0, L)\n",
    "    ptype = _sample_perturbation_type(perturbation_probs)\n",
    "\n",
    "    # 1) 텍스트 삽입\n",
    "    new_steps, ins = create_perturbed_steps(steps, ptype, insert_pos)\n",
    "    e[\"completion\"] = new_steps\n",
    "    e[\"perturbation\"] = ptype\n",
    "    e[\"perturbation_pos\"] = ins\n",
    "    e[\"incorrect_mask\"] = [0] * len(new_steps)\n",
    "    e[\"incorrect_mask\"][ins] = 1\n",
    "    if perturbation_probs:\n",
    "        e[\"perturbation_probs_used\"] = perturbation_probs\n",
    "\n",
    "    # 2) 모든 수치형 벡터에 min_raw_value 삽입 (mi_norm 제외)\n",
    "    numeric_keys = []\n",
    "    for k, v in list(e.items()):\n",
    "        if k == \"mi_norm\":\n",
    "            continue\n",
    "        if _is_numeric_list(v, L):\n",
    "            numeric_keys.append(k)\n",
    "\n",
    "    for k in numeric_keys:\n",
    "        old = [float(x) for x in e[k]]\n",
    "        e[k] = old[:ins] + [float(min_raw_value)] + old[ins:]\n",
    "\n",
    "    # 3) mi_norm 재계산 (주입 스텝 무시하고 통계 산출 → 주입 스텝은 바닥값 강제)\n",
    "    base_priority = [\"mi_loo\", \"mi_shapley\", \"mi_margin\"]  # \"ori_rewards\", \"contributions\", \"cmi\"\n",
    "    base_key = next((k for k in base_priority if k in e and _is_numeric_list(e[k], len(new_steps))), None)\n",
    "\n",
    "    if base_key is not None:\n",
    "        nk = norm_kwargs or {}\n",
    "        e[\"mi_norm\"] = recompute_mi_norm_with_ignore(\n",
    "            e[base_key],\n",
    "            e[\"incorrect_mask\"],\n",
    "            mode=norm_mode_for_mi_norm,\n",
    "            tau=nk.get(\"tau\", 1.5),\n",
    "            clip_z=nk.get(\"clip_z\", 3.0),\n",
    "            deadzone=nk.get(\"deadzone\", 0.2),\n",
    "            q_low=nk.get(\"q_low\", 5.0),\n",
    "            q_high=nk.get(\"q_high\", 95.0),\n",
    "            round_to=nk.get(\"round_to\", 4),\n",
    "        )\n",
    "        e[\"norm_mode\"] = norm_mode_for_mi_norm\n",
    "        e[\"norm_kwargs\"] = nk\n",
    "    return e\n",
    "\n",
    "def process_dataset_copy(data: List[Dict[str, Any]],*,min_raw_value: float = -1e6, norm_mode_for_mi_norm: str = \"signed\",\n",
    "    norm_kwargs: Optional[Dict[str, Any]] = None, perturbation_probs: Optional[Dict[str, float]] = None, seed: int = 42,) -> (List[Dict[str, Any]], Dict[str, Any]):\n",
    "    \n",
    "    random.seed(seed)\n",
    "    out = []\n",
    "    stats = {\"total\": 0, \"types\": Counter(), \"positions\": Counter()}\n",
    "    for e in data:\n",
    "        ne = inject_incorrect_step( e,min_raw_value=min_raw_value, norm_mode_for_mi_norm=norm_mode_for_mi_norm, norm_kwargs=norm_kwargs, perturbation_probs=perturbation_probs,)\n",
    "        out.append(ne)\n",
    "        stats[\"total\"] += 1\n",
    "        stats[\"types\"][ne[\"perturbation\"]] += 1\n",
    "        stats[\"positions\"][ne[\"perturbation_pos\"]] += 1\n",
    "    # Counter -> dict\n",
    "    stats[\"types\"] = dict(stats[\"types\"])\n",
    "    stats[\"positions\"] = dict(stats[\"positions\"])\n",
    "    return out, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= File I/O helpers =======================\n",
    "def print_stats(stats: Dict[str, Any]):\n",
    "    print(f\"Processed entries: {stats.get('total', 0)}\")\n",
    "    print(\"Type distribution:\")\n",
    "    for k, v in sorted(stats.get(\"types\", {}).items()):\n",
    "        print(f\"  - {k}: {v}\")\n",
    "    print(\"Insert position distribution (0-based after renumbering):\")\n",
    "    for k, v in sorted(stats.get(\"positions\", {}).items(), key=lambda x: int(x[0])):\n",
    "        print(f\"  - pos {k}: {v}\")\n",
    "\n",
    "def process_file_copy(input_path: str, output_path: str, **kwargs):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    assert isinstance(data, list), \"Top-level JSON must be a list\"\n",
    "    new_data, stats = process_dataset_copy(data, **kwargs)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(new_data, f, ensure_ascii=False, indent=2)\n",
    "    print_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed entries: 2685\n",
      "Type distribution:\n",
      "  - irrelevant: 1116\n",
      "  - self_reflection: 500\n",
      "  - wrong_step: 1069\n",
      "Insert position distribution (0-based after renumbering):\n",
      "  - pos 0: 642\n",
      "  - pos 1: 642\n",
      "  - pos 2: 666\n",
      "  - pos 3: 396\n",
      "  - pos 4: 194\n",
      "  - pos 5: 97\n",
      "  - pos 6: 29\n",
      "  - pos 7: 15\n",
      "  - pos 8: 2\n",
      "  - pos 9: 2\n"
     ]
    }
   ],
   "source": [
    "process_file_copy(\n",
    "    input_path=\"/home/leena/ccc_eval/mcts_prm/cmi_samples/test_json.json\",\n",
    "    output_path=\"/home/leena/ccc_eval/mcts_prm/cmi_samples/test_incorr.json\",\n",
    "    min_raw_value=-1e6,                          # -5 대신 더 강하게도 OK. (정규화 통계에서 무시하므로 안전)\n",
    "    norm_mode_for_mi_norm=\"unit\",                # \"signed\" | \"unit\" | \"minmax\" | \"relu\"\n",
    "    norm_kwargs={\"tau\":1.5, \"clip_z\":3.0, \"deadzone\":0.2, \"q_low\":5, \"q_high\":95, \"round_to\":4},\n",
    "    perturbation_probs={\"wrong_step\":0.4, \"irrelevant\":0.4, \"self_reflection\":0.2},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys, json\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from prm_training.dataset import PRMDataset, PRMPackCollator\n",
    "from prm_training.wrapper import PRMRewardWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset Debug Start ---\n",
      "\n",
      "--- Sample 0 ---\n",
      "Decoded Text:\n",
      "You are a math expert. Solve the problem step by step.\n",
      "\n",
      "Problem: Let \\[f(x) = \\left\\{\n",
      "\\begin{array}{cl} ax+3, &\\text{ if }x>2, \\\\\n",
      "x-5 &\\text{ if } -2 \\le x \\le 2, \\\\\n",
      "2x-b &\\text{ if } x <-2.\n",
      "\\end{array}\n",
      "\\right.\\]Find $a+b$ if the piecewise function is continuous (which means that its graph can be drawn without lifting your pencil from the paper).\n",
      "Step 1: For the piecewise function to be continuous, the cases must \"meet\" at $2$ and $-2$\n",
      "<RW>\n",
      "Step 2: For example, $ax+3$ and $x-5$ must be equal when $x=2$\n",
      "<RW>\n",
      "Step 3: This implies $a(2)+3=2-5$, which we solve to get $2a=-6 \\Rightarrow a=-3$\n",
      "<RW>\n",
      "Step 4: Similarly, $x-5$ and $2x-b$ must be equal when $x=-2$\n",
      "<RW>\n",
      "Step 5: Substituting, we get $-2-5=2(-2)-b$, which implies $b=3$\n",
      "<RW>\n",
      "Step 6: d/dx (x^2) = 2\n",
      "<RW>\n",
      "Step 7: So $a+b=-3+3=$\n",
      "<RW><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "\n",
      "RW Positions: [147, 176, 211, 239, 270, 288, 304]\n",
      "Targets: [0.3237, 0.4017, 0.5815, 0.4185, 0.9767, 0.0, 0.9775]\n",
      "  - Pos 147, Token '<RW>', Target Reward: 0.3237\n",
      "  - Pos 176, Token '<RW>', Target Reward: 0.4017\n",
      "  - Pos 211, Token '<RW>', Target Reward: 0.5815\n",
      "  - Pos 239, Token '<RW>', Target Reward: 0.4185\n",
      "  - Pos 270, Token '<RW>', Target Reward: 0.9767\n",
      "  - Pos 288, Token '<RW>', Target Reward: 0.0000\n",
      "  - Pos 304, Token '<RW>', Target Reward: 0.9775\n",
      "\n",
      "--- Sample 1 ---\n",
      "Decoded Text:\n",
      "Think step by step and show your reasoning.\n",
      "\n",
      "Problem: A rectangular band formation is a formation with $m$ band members in each of $r$ rows, where $m$ and $r$ are integers. A particular band has less than 100 band members. The director arranges them in a rectangular formation and finds that he has two members left over. If he increases the number of members in each row by 1 and reduces the number of rows by 2, there are exactly enough places in the new formation for each band member. What is the largest number of members the band could have?\n",
      "Step 1: Let $x$ be the number of band members in each row for the original formation, when two are left over\n",
      "<RW>\n",
      "Step 2: Then we can write two equations from the given information: $$rx+2=m$$ $$(r-2)(x+1)=m$$ Setting these equal, we find: $$rx+2=(r-2)(x+1)=rx-2x+r-2$$ $$2=-2x+r-2$$ $$4=r-2x$$ We know that the band has less than 100 members\n",
      "<RW>\n",
      "Step 3: Based on the first equation, we must have $rx$ less than 98\n",
      "<RW>\n",
      "Step 4: We can guess and check some values of $r$ and $x$ in the last equation\n",
      "<RW>\n",
      "Step 5: 2^2 = 5\n",
      "<RW>\n",
      "Step 6: If $r=18$, then $x=7$, and $rx=126$ which is too big\n",
      "<RW>\n",
      "Step 7: If $r=16$, then $x=6$, and $rx=96$, which is less than 98\n",
      "<RW>\n",
      "Step 8: Checking back in the second formation, we see that $(16-2)(6+1)=14\\cdot 7=98$ as it should\n",
      "<RW>\n",
      "Step 9: This is the best we can do, so the largest number of members the band could have is $$\n",
      "<RW><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "\n",
      "RW Positions: [152, 245, 269, 295, 309, 341, 375, 415, 442]\n",
      "Targets: [0.5, 0.5, 0.5, 0.2827, 0.0, 0.2126, 0.2339, 0.5481, 0.9472]\n",
      "  - Pos 152, Token '<RW>', Target Reward: 0.5000\n",
      "  - Pos 245, Token '<RW>', Target Reward: 0.5000\n",
      "  - Pos 269, Token '<RW>', Target Reward: 0.5000\n",
      "  - Pos 295, Token '<RW>', Target Reward: 0.2827\n",
      "  - Pos 309, Token '<RW>', Target Reward: 0.0000\n",
      "  - Pos 341, Token '<RW>', Target Reward: 0.2126\n",
      "  - Pos 375, Token '<RW>', Target Reward: 0.2339\n",
      "  - Pos 415, Token '<RW>', Target Reward: 0.5481\n",
      "  - Pos 442, Token '<RW>', Target Reward: 0.9472\n",
      "\n",
      "--- Sample 2 ---\n",
      "Decoded Text:\n",
      "Solve carefully, one step at a time.\n",
      "\n",
      "Problem: What is the degree of the polynomial $(4 +5x^3 +100 +2\\pi x^4 + \\sqrt{10}x^4 +9)$?\n",
      "Step 1: This coffee tastes a bit strong.\n",
      "<RW>\n",
      "Step 2: This polynomial is not written in standard form\n",
      "<RW>\n",
      "Step 3: However, we don't need to write it in standard form, nor do we need to pay attention to the coefficients\n",
      "<RW>\n",
      "Step 4: We just look for the exponents on $x$\n",
      "<RW>\n",
      "Step 5: We have an $x^4$ term and no other term of higher degree, so $$ is the degree of the polynomial\n",
      "<RW><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "\n",
      "RW Positions: [62, 77, 107, 124, 156]\n",
      "Targets: [0.0, 0.4229, 0.5771, 0.3057, 0.8012]\n",
      "  - Pos 62, Token '<RW>', Target Reward: 0.0000\n",
      "  - Pos 77, Token '<RW>', Target Reward: 0.4229\n",
      "  - Pos 107, Token '<RW>', Target Reward: 0.5771\n",
      "  - Pos 124, Token '<RW>', Target Reward: 0.3057\n",
      "  - Pos 156, Token '<RW>', Target Reward: 0.8012\n",
      "\n",
      "--- Dataset Debug End ---\n"
     ]
    }
   ],
   "source": [
    "def debug_dataset(dataset: PRMDataset, tokenizer, num_samples=3):\n",
    "    print(\"--- Dataset Debug Start ---\")\n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        print(f\"\\n--- Sample {i} ---\")\n",
    "        \n",
    "        decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)\n",
    "        print(f\"Decoded Text:\\n{decoded_text}\")\n",
    "        \n",
    "        rw_positions = sample['rw_positions']\n",
    "        targets = sample['targets']\n",
    "        \n",
    "        print(f\"\\nRW Positions: {rw_positions}\")\n",
    "        print(f\"Targets: {targets}\")\n",
    "        \n",
    "        # 각 RW 토큰 위치와 보상 값을 확인\n",
    "        tokens = tokenizer.convert_ids_to_tokens(sample['input_ids'])\n",
    "        for pos, target in zip(rw_positions, targets):\n",
    "            print(f\"  - Pos {pos}, Token '{tokens[pos]}', Target Reward: {target:.4f}\")\n",
    "    print(\"\\n--- Dataset Debug End ---\")\n",
    "\n",
    "def debug_schema(ds, nprint=3):\n",
    "        n = len(ds)\n",
    "        bad = []\n",
    "        for i in range(n):\n",
    "            s = ds[i]\n",
    "            if \"rw_positions\" not in s or \"targets\" not in s:\n",
    "                bad.append(i)\n",
    "        print(f\"[schema] total={n}, pack_ok={n-len(bad)}, missing_pack_keys={len(bad)}\")\n",
    "        if bad[:nprint]:\n",
    "            print(\"examples of bad idx:\", bad[:nprint])\n",
    "            for j in bad[:nprint]:\n",
    "                print(ds[j].keys())\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-Math-7B\", trust_remote_code=True, use_fast=False)\n",
    "data_path = \"/home/leena/ccc_eval/mcts_prm/cmi_samples/test_incorr.json\"\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    entries = json.load(f)\n",
    "\n",
    "full_ds = PRMDataset(entries, tok, reward_key=\"mi_loo\", max_length=512)\n",
    "debug_dataset(full_ds, tok, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  2610,    525,    264,   6888,   6203,     13,  63284,    279,   3491,\n",
       "           3019,    553,   3019,    382,  31198,     25,   6771,   1124,     58,\n",
       "             69,   2075,      8,    284,   1124,   2359,     59,    515,     59,\n",
       "           7265,     90,   1653,  15170,    564,     92,   3859,     10,     18,\n",
       "             11,    609,     59,   1318,     90,    421,    335,     87,     29,\n",
       "             17,     11,  90155,     87,     12,     20,    609,     59,   1318,\n",
       "             90,    421,    335,    481,     17,   1124,    273,    856,   1124,\n",
       "            273,    220,     17,     11,  90155,     17,     87,   1455,    609,\n",
       "             59,   1318,     90,    421,    335,    856,   9119,     17,    624,\n",
       "             59,    408,     90,   1653,    532,     59,   1291,   7110,     60,\n",
       "           9885,    400,     64,  35093,      3,    421,    279,   6573,   4482,\n",
       "            729,    374,  19259,    320,   8206,   3363,    429,   1181,   4771,\n",
       "            646,    387,  14764,   2041,  32410,    697,  46118,    504,    279,\n",
       "           5567,   4292,   8304,    220,     16,     25,   1752,    279,   6573,\n",
       "           4482,    729,    311,    387,  19259,     11,    279,   5048,   1969,\n",
       "            330,  63410,      1,    518,    400,     17,      3,    323,    400,\n",
       "             12,     17,  25046, 151665,    198,   8304,    220,     17,     25,\n",
       "           1752,   3110,     11,    400,    706,     10,     18,      3,    323,\n",
       "            400,     87,     12,     20,      3,   1969,    387,   6144,    979,\n",
       "            400,     87,     28,     17,  25046, 151665,    198,   8304,    220,\n",
       "             18,     25,   1096,  23945,    400,     64,      7,     17,   7257,\n",
       "             18,     28,     17,     12,     20,  54876,    892,    582,  11625,\n",
       "            311,    633,    400,     17,     64,  10829,     21,   1124,  26243,\n",
       "            264,  10829,     18,  25046, 151665,    198,   8304,    220,     19,\n",
       "             25,  34239,     11,    400,     87,     12,     20,      3,    323,\n",
       "            400,     17,     87,   1455,      3,   1969,    387,   6144,    979,\n",
       "            400,     87,  10829,     17,  25046, 151665,    198,   8304,    220,\n",
       "             20,     25,   3719,   3696,  10607,     11,    582,    633,    400,\n",
       "             12,     17,     12,     20,     28,     17,   4080,     17,   7287,\n",
       "             65,  54876,    892,  23945,    400,     65,     28,     18,  25046,\n",
       "         151665,    198,   8304,    220,     21,     25,    294,   3446,     87,\n",
       "            320,     87,     61,     17,      8,    284,    220,     17,    198,\n",
       "         151665,    198,   8304,    220,     22,     25,   2055,    400,     64,\n",
       "          35093,  10829,     18,     10,     18,   3186,    198, 151665, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
       "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'rw_positions': [147, 176, 211, 239, 270, 288, 304],\n",
       " 'targets': [0.3237, 0.4017, 0.5815, 0.4185, 0.9767, 0.0, 0.9775],\n",
       " 'is_incorrect': [0, 0, 0, 0, 0, 1, 0],\n",
       " 'meta': {'q': 'Let \\\\[f(x) = \\\\left\\\\{\\n\\\\begin{array}{cl} ax+3, &\\\\text{ if }x>2, \\\\\\\\\\nx-5 &\\\\text{ if } -2 \\\\le x \\\\le 2, \\\\\\\\\\n2x-b &\\\\text{ if } x <-2.\\n\\\\end{array}\\n\\\\right.\\\\]Find $a+b$ if the piecewise function is continuous (which means that its graph can be drawn without lifting your pencil from the paper).',\n",
       "  'n_steps': 7}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_dir = \"/path/to/final_model\"\n",
    "tok = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "prm = PRMRewardWrapper.from_pretrained(\n",
    "    model_dir,\n",
    "    tokenizer=tok,                 # 있으면 rw_token_id 자동 설정\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# 예시 입력(이미 각 step 뒤에 <RW>가 삽입된 텍스트라고 가정)\n",
    "text = \"Problem: ...\\nStep 1: ...\\n<RW>\\nStep 2: ...\\n<RW>\\n\"\n",
    "enc = tok(text, return_tensors=\"pt\")\n",
    "for k in enc:\n",
    "    enc[k] = enc[k].to(prm.backbone.device)\n",
    "\n",
    "rewards = prm.predict_rewards_at_rw(enc[\"input_ids\"], enc[\"attention_mask\"])  # list[Tensor]\n",
    "print([r.detach().float().cpu().tolist() for r in rewards])  # 각 <RW> 위치의 스칼라 보상들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer Fintuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leena/anaconda3/envs/prm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FTPRM(nn.Module):\n",
    "    def __init__(self, base_model_name: str, lora_rank: int = 16, lora_alpha: int = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = AutoModel.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if hasattr(self.backbone, \"score\"):\n",
    "            # For Qwen2.5-Math-PRM-7B\n",
    "            in_feat = self.backbone.score[0].in_features\n",
    "            self.backbone.score = nn.Sequential(\n",
    "                nn.Linear(in_feat, in_feat),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_feat, 1, bias=True)  # 2 → 1\n",
    "            )\n",
    "            self.reg_head = None\n",
    "        else:\n",
    "            # Other AutoModel(Causal LM)\n",
    "            hidden = self.backbone.config.hidden_size\n",
    "            self.reg_head = nn.Sequential(\n",
    "                nn.Linear(hidden, hidden // 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(hidden // 4, 1)\n",
    "            )\n",
    "\n",
    "        # Add Lora Adapter\n",
    "        self.backbone = prepare_model_for_kbit_training(self.backbone)\n",
    "        lora_cfg = LoraConfig(\n",
    "            r=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        self.backbone = get_peft_model(self.backbone, lora_cfg)\n",
    "        self._activate_head_params()   \n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels=None):\n",
    "        out = self.backbone(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            output_hidden_states=True,\n",
    "                            return_dict=True)\n",
    "        hidden = out.hidden_states[-1]                     # (B, L, H)\n",
    "\n",
    "        # Last token vector\n",
    "        if attention_mask is None:   \n",
    "            rep = hidden[:, -1, :]\n",
    "        else:\n",
    "            seq_len = attention_mask.sum(1) - 1           # (B,)\n",
    "            rep = hidden[torch.arange(hidden.size(0), device=hidden.device), seq_len, :]\n",
    "\n",
    "        # head 통과\n",
    "        if self.reg_head is None:\n",
    "            pred = self.backbone.score(rep).squeeze(-1)\n",
    "        else:\n",
    "            pred = self.reg_head(rep).squeeze(-1)\n",
    "\n",
    "        if labels is not None:              # training / eval\n",
    "            loss = F.mse_loss(pred, labels.float())\n",
    "            return loss, pred   \n",
    "        else:                               # pure inference\n",
    "            return pred\n",
    "\n",
    "    def _activate_head_params(self):\n",
    "        if self.reg_head is not None:\n",
    "            for p in self.reg_head.parameters():\n",
    "                p.requires_grad_(True)\n",
    "        else:\n",
    "            for p in self.backbone.score.parameters():\n",
    "                p.requires_grad_(True)\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        return [p for p in self.parameters() if p.requires_grad]\n",
    "    \n",
    "    def get_parameter_stats(self):\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        module_stats = {}\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "                \n",
    "                module_name = name.split('.')[0]\n",
    "                if module_name not in module_stats:\n",
    "                    module_stats[module_name] = {'trainable': 0, 'total': 0}\n",
    "                module_stats[module_name]['trainable'] += param.numel()\n",
    "                module_stats[module_name]['total'] += param.numel()\n",
    "            else:\n",
    "                module_name = name.split('.')[0]\n",
    "                if module_name not in module_stats:\n",
    "                    module_stats[module_name] = {'trainable': 0, 'total': 0}\n",
    "                module_stats[module_name]['total'] += param.numel()\n",
    "        \n",
    "        return {\n",
    "            'total_params': all_param,\n",
    "            'trainable_params': trainable_params,\n",
    "            'trainable_ratio': trainable_params / all_param * 100,\n",
    "            'module_stats': module_stats\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.72s/it]\n",
      "Some weights of the model checkpoint at Qwen/Qwen2.5-Math-PRM-7B were not used when initializing Qwen2ForProcessRewardModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FTPRM(\n",
       "  (backbone): PeftModel(\n",
       "    (base_model): LoraModel(\n",
       "      (model): Qwen2ForProcessRewardModel(\n",
       "        (model): Qwen2Model(\n",
       "          (embed_tokens): Embedding(152064, 3584)\n",
       "          (layers): ModuleList(\n",
       "            (0-27): 28 x Qwen2DecoderLayer(\n",
       "              (self_attn): Qwen2SdpaAttention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (rotary_emb): Qwen2RotaryEmbedding()\n",
       "              )\n",
       "              (mlp): Qwen2MLP(\n",
       "                (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "                (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "                (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "              (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (norm): Qwen2RMSNorm((3584,), eps=1e-05)\n",
       "        )\n",
       "        (score): Sequential(\n",
       "          (0): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=3584, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-Math-PRM-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = FTPRM(base_model_name=model_name)\n",
    "model.to(device)\n",
    "# model.get_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# PRM Custom Class\n",
    "class FTLM(nn.Module):\n",
    "    def __init__(self, base_model_name: str, lora_rank: int = 16, lora_alpha: int = 32, mlp_ratio: int = 4, value_head_prefix: str = \"value_head\", \n",
    "                 normalize_reward: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Use AutoModelForCausalLM for pure CausalLM fine-tuning\n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            quantization_config=BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Add Lora Adapter for efficient fine-tuning\n",
    "        self.backbone = prepare_model_for_kbit_training(self.backbone)\n",
    "        lora_cfg = LoraConfig(\n",
    "            r=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        self.backbone = get_peft_model(self.backbone, lora_cfg)\n",
    "\n",
    "        # Value head for reward prediction\n",
    "        hidden = self.backbone.config.hidden_size\n",
    "        mlp_hidden = hidden // mlp_ratio\n",
    "        head = nn.Sequential(\n",
    "            nn.Linear(hidden, mlp_hidden, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(mlp_hidden, 1, bias=False),\n",
    "        )\n",
    "        self.value_head_prefix = value_head_prefix\n",
    "        setattr(self, value_head_prefix, head)\n",
    "\n",
    "        # head 가중치 학습 가능하도록 보장\n",
    "        for p in head.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        \n",
    "        self.normalize_reward = normalize_reward\n",
    "        self.register_buffer(\"mean\", torch.zeros(1), persistent=False)\n",
    "        self.register_buffer(\"std\",  torch.ones(1),  persistent=False)\n",
    "\n",
    "        # 캐시 비활성 (gradient checkpointing 호환)\n",
    "        self.backbone.config.use_cache = False\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels=None, return_hidden: bool = False):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = (input_ids != self.backbone.config.pad_token_id).long()\n",
    "\n",
    "        # position_ids = cumulative mask\n",
    "        position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "        position_ids.masked_fill_(attention_mask == 0, 0)\n",
    "\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            output_hidden_states=True,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "        last_hidden = outputs.hidden_states[-1]        # (B, T, H)\n",
    "        # Index of last non-pad token  → (B, 1)\n",
    "        eos_idx = attention_mask.size(1) - 1 - attention_mask.long().fliplr().argmax(-1, keepdim=True)\n",
    "\n",
    "        values = getattr(self, self.value_head_prefix)(last_hidden).squeeze(-1)   # (B, T)\n",
    "        reward = values.gather(1, eos_idx).squeeze(1)                             # (B,)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = F.mse_loss(reward, labels.float())\n",
    "            return loss, reward\n",
    "        else:\n",
    "            # if (not self.training) and self.normalize_reward:\n",
    "            #     reward = (reward - self.mean) / (self.std + 1e-8)\n",
    "            return (reward, last_hidden) if return_hidden else reward\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        return [p for p in self.parameters() if p.requires_grad]\n",
    "    \n",
    "    def get_parameter_stats(self):\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        module_stats = {}\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "                \n",
    "                module_name = name.split('.')[0]\n",
    "                if module_name not in module_stats:\n",
    "                    module_stats[module_name] = {'trainable': 0, 'total': 0}\n",
    "                module_stats[module_name]['trainable'] += param.numel()\n",
    "                module_stats[module_name]['total'] += param.numel()\n",
    "            else:\n",
    "                module_name = name.split('.')[0]\n",
    "                if module_name not in module_stats:\n",
    "                    module_stats[module_name] = {'trainable': 0, 'total': 0}\n",
    "                module_stats[module_name]['total'] += param.numel()\n",
    "        \n",
    "        return {\n",
    "            'total_params': all_param,\n",
    "            'trainable_params': trainable_params,\n",
    "            'trainable_ratio': trainable_params / all_param * 100,\n",
    "            'module_stats': module_stats\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 4,366,276,992\n",
      "Trainable parameters: 13,304,704\n",
      "Trainable ratio: 0.30%\n",
      "FTLM(\n",
      "  (backbone): PeftModel(\n",
      "    (base_model): LoraModel(\n",
      "      (model): Qwen2ForCausalLM(\n",
      "        (model): Qwen2Model(\n",
      "          (embed_tokens): Embedding(152064, 3584)\n",
      "          (layers): ModuleList(\n",
      "            (0-27): 28 x Qwen2DecoderLayer(\n",
      "              (self_attn): Qwen2Attention(\n",
      "                (q_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (v_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): lora.Linear4bit(\n",
      "                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "              (mlp): Qwen2MLP(\n",
      "                (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "                (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "                (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "              (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "            )\n",
      "          )\n",
      "          (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "          (rotary_emb): Qwen2RotaryEmbedding()\n",
      "        )\n",
      "        (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (value_head): Sequential(\n",
      "    (0): Linear(in_features=3584, out_features=896, bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=896, out_features=1, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = FTLM(base_model_name=model_name, value_head_prefix=\"value_head\")\n",
    "model.to(device)\n",
    "\n",
    "stats = model.get_parameter_stats()\n",
    "print(f\"Total parameters: {stats['total_params']:,}\")\n",
    "print(f\"Trainable parameters: {stats['trainable_params']:,}\")\n",
    "print(f\"Trainable ratio: {stats['trainable_ratio']:.2f}%\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PRMCollator:\n",
    "    tokenizer: AutoTokenizer\n",
    "    pad_to_multiple_of: Optional[int] = 8\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        input_ids, rewards = zip(*batch)\n",
    "        lengths = [len(ids) for ids in input_ids]\n",
    "        max_len = max(lengths)\n",
    "        if self.pad_to_multiple_of:\n",
    "            max_len = int(math.ceil(max_len / self.pad_to_multiple_of) * self.pad_to_multiple_of)\n",
    "\n",
    "        padded = [\n",
    "            torch.cat([ids, ids.new_full((max_len - len(ids),), self.tokenizer.pad_token_id)])\n",
    "            for ids in input_ids\n",
    "        ]\n",
    "        input_ids = torch.stack(padded)\n",
    "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": rewards,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward type: cmi\n",
      "Full dataset size: 26720\n",
      "Finish Loading Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Loading Trainer\n",
      "loss: 0.6680973768234253 pred shape: torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 3830919681,\n",
       " 'trainable_params': 22944769,\n",
       " 'trainable_ratio': 0.598936310614861,\n",
       " 'module_stats': {'backbone': {'trainable': 22944769, 'total': 3830919681}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from prm_dataset import StepwisePRMDataset\n",
    "from config import PRMConfig\n",
    "\n",
    "with open(\"/home/leena/ccc_eval/mcts_prm/cmi_samples/total_gsm8k_merge_mistral.json\", \"r\") as file:\n",
    "    gsm8k_raw = json.load(file)\n",
    "\n",
    "cfg = PRMConfig()\n",
    "full_ds = StepwisePRMDataset(gsm8k_raw, tokenizer, cfg.max_new_tokens, reward_type=\"cmi\")\n",
    "print(f\"Full dataset size: {len(full_ds)}\") \n",
    "\n",
    "indices = list(range(len(full_ds)))\n",
    "split_idx = int(0.9 * len(full_ds)) if len(full_ds) > 1 else 1\n",
    "train_indices = indices[:split_idx]\n",
    "val_indices = indices[split_idx:] if len(full_ds) > 1 else indices[:1]\n",
    "\n",
    "train_ds = Subset(full_ds, train_indices)\n",
    "valid_ds = Subset(full_ds, val_indices)\n",
    "\n",
    "collate = PRMCollator(tokenizer)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=16, shuffle=False, collate_fn=collate)\n",
    "print(\"Finish Loading Dataset\")\n",
    "\n",
    "output_dir = \"/home/leena/ccc_eval/mcts_prm/prm_training/checkpoints/pt_prm\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,       # ← 변수명 교정\n",
    "    data_collator=collate        # ← 반드시 추가\n",
    ")\n",
    "print(\"Finish Loading Trainer\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    b = collate([train_ds[i] for i in range(4)])\n",
    "    out = model(**{k: v.to(device) for k, v in b.items()})\n",
    "    print(\"loss:\", out[0].item(), \"pred shape:\", out[1].shape)\n",
    "\n",
    "model.get_parameter_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step_rewards(model, tokenizer, problem, steps):\n",
    "    \"\"\"\n",
    "    problem: str\n",
    "    steps  : List[str]  # [\"Step 1: ...\", \"Step 2: ...\", ...]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    prefix = [f\"Problem: {problem}\"]\n",
    "    rewards = []\n",
    "    for s in steps:\n",
    "        prefix.append(s)\n",
    "        txt = \"\\n\".join(prefix)\n",
    "        enc = tokenizer(txt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            reward = model(**enc).item()      # forward returns pred\n",
    "        rewards.append(reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QwQ prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Project-level helpers\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m current_dir = Path(\u001b[34;43m__file__\u001b[39;49m).parent\n\u001b[32m     12\u001b[39m project_root = current_dir.parent\n\u001b[32m     13\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(project_root))\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re, sys\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from pathlib import Path\n",
    "\n",
    "# Project-level helpers\n",
    "current_dir = Path(__file__).parent\n",
    "project_root = current_dir.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"3\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from config import PRMConfig\n",
    "from utils import _sanitize_enhanced, _numeric_equiv_enhanced, _extract_boxed_answer, system_prompt\n",
    "from inference.answer_extractor import AnswerExtractor\n",
    "from inference.answer_matcher import MathAnswerScorer\n",
    "\n",
    "def _format_system_prompt() -> str:\n",
    "    return (\n",
    "        \"You are an **expert mathematical-reasoning assistant**.\\n\\n\"\n",
    "        \"## Format rules\\n\"\n",
    "        \"1. Begin *every* reasoning line with the exact prefix `Step k:` where `k = 1, 2, …`. No other prefix is allowed.\\n\"\n",
    "        \"2. Show *all* intermediate calculations using standard symbols (×, ÷, ±, √).\\n\"\n",
    "        \"3. Put your final answer within `Answer: \\boxed{}`. and **stop immediately** — no extra text after the answer.\\n\"\n",
    "        \"4. Each step must be concise *yet mathematically rigorous*.\\n\"\n",
    "        \"5. Do not generate any text or reflection if you reach the final answer.\\n\\n\"\n",
    "        \"Follow these rules exactly — evaluations are case- and format‑sensitive.\\n\"\n",
    "        \"Respond *only* in the specified format.\"\n",
    "    )\n",
    "\n",
    "def build_chat_messages_qwq(*, question: str, tokenizer, dataset: str, shots: Optional[List[Tuple[str, str, str]]] = None, prefix_context: Optional[str] = None, next_label: Optional[str] = None,) -> str:\n",
    "    system_prompt = _format_system_prompt()\n",
    "    default_shots: List[Tuple[str, str, str]] = [\n",
    "        (\n",
    "            \"gsm8k, math, olympiad, omni\",\n",
    "            \"Problem: What is the next number in the sequence 2, 4, 8, 16?\",\n",
    "            \"Step 1: Identify the pattern; each term is multiplied by 2.\\n\"\n",
    "            \"Step 2: 16 × 2 = 32\\n\"\n",
    "            \"Answer: 32\",\n",
    "        ),\n",
    "        (\n",
    "            \"gsm8k, math\",\n",
    "            \"Problem: Solve for x: 3x + 7 = 22\",\n",
    "            \"Step 1: Subtract 7 from both sides: 3x = 15\\n\"\n",
    "            \"Step 2: Divide by 3: x = 5\\n\"\n",
    "            \"Answer: 5\",\n",
    "        ),\n",
    "        (\n",
    "            \"olympiad, omni\",\n",
    "            \"Problem: Determine whether v₁ = [1,2] and v₂ = [3,6] are linearly independent.\",\n",
    "            \"Step 1: Observe v₂ = 3 · v₁, so v₂ is a scalar multiple of v₁.\\n\"\n",
    "            \"Step 2: Therefore the vectors are linearly dependent.\\n\"\n",
    "            \"Answer: Dependent\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    if shots is None:\n",
    "        shots = default_shots\n",
    "    user_lines: List[str] = []\n",
    "    \n",
    "    if prefix_context:\n",
    "        user_lines.append(prefix_context.rstrip())\n",
    "    \n",
    "    user_lines.append(f\"Problem: {question}\".rstrip())\n",
    "    if next_label:\n",
    "        user_lines.append(next_label.rstrip())\n",
    "    user_content = \"\\n\".join([ln for ln in user_lines if ln])\n",
    "\n",
    "    messages: List[Dict[str, str]] = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "    for tag, q, a in shots:\n",
    "        if dataset.lower() in tag.lower():\n",
    "            messages.append({\"role\": \"user\", \"content\": q})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": a})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "    return tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "def build_masking_chat_messages_qwq(tokenizer, sentence: str) -> str:\n",
    "    masking_system = (\n",
    "        \"In the sentence below, mask any word or expression that seems crucial \"\n",
    "        \"(such as a variable, a number or an operator, etc.) for solving the math problem \"\n",
    "        \"by replacing it with '[MASKED]'.\"\n",
    "    )\n",
    "    user_content = f\"Sentence: \\\"{sentence}\\\"\\nRewritten:\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": masking_system},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "class ContriRewardvLLM:\n",
    "    def __init__(self, config: \"PRMConfig\", model_name: str = \"mistralai/Mathstral-7B-v0.1\"):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.llm = LLM(\n",
    "            model=model_name,\n",
    "            trust_remote_code=True,\n",
    "            dtype=\"bfloat16\",\n",
    "            gpu_memory_utilization=0.8,\n",
    "            max_model_len=4096,\n",
    "            quantization=\"bitsandbytes\",\n",
    "        )\n",
    "        self.tokenizer = self.llm.get_tokenizer()\n",
    "\n",
    "        self.scorer = MathAnswerScorer()\n",
    "        self.extractor = AnswerExtractor()\n",
    "        \n",
    "        self.rollout_params = SamplingParams(\n",
    "            temperature=0.6,\n",
    "            top_p=0.95,\n",
    "            max_tokens=self.config.max_new_tokens,\n",
    "            n=self.config.num_rollouts,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "        self.masking_params = SamplingParams(\n",
    "            temperature=0.6,\n",
    "            top_p=0.95,\n",
    "            max_tokens=self.config.max_new_tokens,\n",
    "            n=self.config.num_rollouts,\n",
    "            repetition_penalty=1.1,\n",
    "        )\n",
    "        print(f\"vLLM model loaded: {model_name}\")\n",
    "\n",
    "    def _batched_generate(self, prompts: List[str], params: SamplingParams):\n",
    "        return self.llm.generate(prompts, params)\n",
    "\n",
    "    def _score_batch(self, outputs, gold_answer: str) -> List[float]:\n",
    "        rewards = []\n",
    "        for result in outputs:\n",
    "            correct = 0\n",
    "            for comp in result.outputs:\n",
    "                text = comp.text or \"\"\n",
    "                tail = text.rsplit(\"Answer:\", 1)[-1] if \"Answer:\" in text else text\n",
    "                pred = self.extractor.extract_pred_answer(tail)\n",
    "                print(\"Prediction Answer:\", pred)\n",
    "                if self.scorer.answers_match(pred, gold_answer):\n",
    "                    correct += 1\n",
    "            rewards.append(correct / float(self.config.num_rollouts))\n",
    "        return rewards\n",
    "\n",
    "    def _make_prompt(self, *, question: str, staged_steps: List[str], next_label: str, dataset: str) -> str:\n",
    "        prefix_context = \"\\n\".join(staged_steps)\n",
    "        return build_chat_messages_qwq(question=question, tokenizer=self.tokenizer, dataset=dataset, prefix_context=prefix_context, next_label=next_label)\n",
    "    \n",
    "    def compute_step_rewards_batch(self, question: str, dataset: str, steps: List[str], gold_answer: str) -> List[float]:\n",
    "        prompts: List[str] = []\n",
    "        for i in range(len(steps)):\n",
    "            next_label = f\"Step {i + 2}:\" if i < len(steps) - 1 else \"Answer:\"\n",
    "            staged_steps = steps[: i + 1]\n",
    "            prompts.append(self._make_prompt(question=question, staged_steps=staged_steps, next_label=next_label, dataset=dataset))\n",
    "        outputs = self._batched_generate(prompts, self.rollout_params)\n",
    "        print(\"Generated rollout outputs:\")\n",
    "        for i, output in enumerate(outputs):\n",
    "            print(f\"Output {i}: {output.outputs[0].text}\")\n",
    "        return self._score_batch(outputs, gold_answer)\n",
    "        \n",
    "    def model_masking_batch(self, texts: List[str]) -> List[str]:\n",
    "        mask_prompts = [build_masking_chat_messages_qwq(self.tokenizer, t) for t in texts]\n",
    "        outputs = self._batched_generate(mask_prompts, self.masking_params)\n",
    "        print(\"Mask Generation:\")\n",
    "        for i, output in enumerate(outputs):\n",
    "            print(f\"Output {i}: {output.outputs[0].text}\")\n",
    "        return [out.outputs[0].text.strip() for out in outputs]\n",
    "\n",
    "    def perturb_step_rewards_batch(self, question: str, dataset: str, steps: List[str], gold_answer: str, use_llm: bool = True) -> List[float]:\n",
    "        bodies = []\n",
    "        prefixes = []\n",
    "        for step in steps:\n",
    "            m = re.match(r\"^[\\s>#*\\-]*Step\\s*\\d+\\s*[:.\\-]\\s*\", step, flags=re.I)\n",
    "            prefixes.append(m.group(0) if m else \"\")\n",
    "            bodies.append(step[len(prefixes[-1]):])\n",
    "\n",
    "        if use_llm:\n",
    "            masked_bodies = self.model_masking_batch(bodies)\n",
    "        else:\n",
    "            masked_bodies = [self._MASK_PATTERN.sub(\"[MASKED]\", b) for b in bodies]\n",
    "        print(\"Masked Body:\", masked_bodies)   \n",
    "        prompts = []\n",
    "        for i in range(len(steps)):\n",
    "            masked_step = prefixes[i] + masked_bodies[i]\n",
    "            staged_steps = steps[:i] + [masked_step]\n",
    "            next_label = f\"Step {i + 2}:\" if i < len(steps) - 1 else \"Answer:\"\n",
    "            prompts.append(self._make_prompt(question=question, staged_steps=staged_steps, next_label=next_label, dataset=dataset))\n",
    "        \n",
    "        outputs = self._batched_generate(prompts, self.rollout_params)\n",
    "        return self._score_batch(outputs, gold_answer)\n",
    "\n",
    "    def gsm8k_reward_dataset_vllm(self, *, split: str = \"train\", start: int = 0, take: Optional[int] = None):\n",
    "        ds = load_dataset(\"openai/gsm8k\", \"main\", split=split)\n",
    "        ds = ds.select(range(start, start + take)) if take else ds\n",
    "        # ds = ds.select(range(start, len(ds)))\n",
    "        # print(\"Generated dataset size: \", len(ds))\n",
    "\n",
    "        for sample in tqdm(ds, desc=\"Building GSM8K contri reward-dataset\"):\n",
    "            q_txt, g_sol = sample[\"question\"], sample[\"answer\"]\n",
    "            lines, gold_ans = [], None\n",
    "            \n",
    "            gold_ans = self.extractor.extract_gold_answer(g_sol, \"gsm8k\")\n",
    "            if gold_ans is None:\n",
    "                raise ValueError(\"gold answer not found for sample\")\n",
    "            \n",
    "            lines = [ln.strip() for ln in g_sol.splitlines() if ln.strip()]\n",
    "            steps = [f\"Step {i+1}: {t}\" for i, t in enumerate(lines)]\n",
    "            # steps = [f\"Step {i+1}: {t}\" for i, t in enumerate(lines) if not t.lower().startswith(\"answer\")] \n",
    "            print(\"Steps Split:\", steps)\n",
    "\n",
    "            ori = self.compute_step_rewards_batch(q_txt, \"gsm8k\", steps, gold_ans)\n",
    "            ptb = self.perturb_step_rewards_batch(q_txt, \"gsm8k\", steps, gold_ans, self.config.use_llm)\n",
    "            print(\"Original Rewards:\", ori)\n",
    "            print(\"Masked Rewards:\", ptb)\n",
    "            contrib = [round(o - p, 4) for o, p in zip(ori, ptb)]\n",
    "\n",
    "            entry = {\n",
    "                \"question\": q_txt,\n",
    "                \"completion\": steps,\n",
    "                \"ori_rewards\": ori,\n",
    "                \"ptb_rewards\": ptb,\n",
    "                \"contributions\": contrib,\n",
    "                \"gold_answer\": gold_ans,\n",
    "            }\n",
    "            yield entry\n",
    "\n",
    "    def math_reward_dataset_vllm(self, *, split: str = \"train\", start: int = 0, take: Optional[int] = None):\n",
    "        sent_split = re.compile(r'\\.(?!\\d)(?=\\s|$)')\n",
    "        ds = load_dataset(\"HuggingFaceTB/MATH\", \"all\", split=split)\n",
    "        ds = ds.select(range(start, start + take)) if take else ds\n",
    "        # ds = ds.select(range(start, len(ds)))\n",
    "        # print(\"Generated dataset size: \", len(ds))\n",
    "        \n",
    "        for sample in tqdm(ds, desc=\"Building MATH contri reward-dataset\"):\n",
    "            full_sol = sample[\"solution\"]\n",
    "            \n",
    "            gold_ans = self.extractor.extract_gold_answer(full_sol, \"math\")\n",
    "            if gold_ans is None:\n",
    "                lines = [line.strip() for line in full_sol.splitlines() if line.strip()]\n",
    "                for line in reversed(lines):\n",
    "                    if re.search(r'[\\d\\-+*/()=]', line):\n",
    "                        gold_ans = _sanitize_enhanced(line)\n",
    "                        break\n",
    "            \n",
    "            sol_wo_box = re.sub(r'\\\\boxed\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}', '', full_sol)\n",
    "            raw_steps = [s.strip() for s in sent_split.split(sol_wo_box) if s.strip()]\n",
    "            steps = [f\"Step {i+1}: {s}\" for i, s in enumerate(raw_steps)]\n",
    "            print(\"Steps Split:\", steps)\n",
    "\n",
    "            ori = self.compute_step_rewards_batch(sample[\"problem\"],\"math\", steps, gold_ans)\n",
    "            ptb = self.perturb_step_rewards_batch(sample[\"problem\"], \"math\", steps, gold_ans, self.config.use_llm)\n",
    "            print(\"Original Rewards:\", ori)\n",
    "            print(\"Masked Rewards:\", ptb)\n",
    "            contrib = [round(o - p, 4) for o, p in zip(ori, ptb)]\n",
    "\n",
    "            entry = {\n",
    "                \"question\": sample[\"problem\"],\n",
    "                \"completion\": steps,\n",
    "                \"ori_rewards\": ori,\n",
    "                \"ptb_rewards\": ptb,\n",
    "                \"contributions\": contrib,\n",
    "                \"gold_answer\": gold_ans,\n",
    "            }\n",
    "            yield entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BCE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "# (옵션) LoRA를 쓰려면 peft import 후 주석 해제\n",
    "# from peft import get_peft_model, LoraConfig\n",
    "\n",
    "class PRMTokenClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LM 마지막 히든(토큰 단위)에 1-logit 헤드를 붙여,\n",
    "    <RW> 위치들만 선택해 BCEWithLogitsLoss(logit, r) 최적화.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model_name: str, tokenizer: AutoTokenizer,\n",
    "                 use_lora: bool = False, freeze_backbone: bool = False):\n",
    "        super().__init__()\n",
    "        self.tok = tokenizer\n",
    "        self.lm  = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name, torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\", trust_remote_code=True\n",
    "        )\n",
    "        # 스페셜 토큰 추가 후 임베딩 리사이즈\n",
    "        self.lm.resize_token_embeddings(len(self.tok))\n",
    "\n",
    "        # (옵션) LoRA\n",
    "        # if use_lora:\n",
    "        #     lora = LoraConfig(\n",
    "        #         r=8, lora_alpha=16, lora_dropout=0.05,\n",
    "        #         target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
    "        #     )\n",
    "        #     self.lm = get_peft_model(self.lm, lora)\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.lm.parameters(): p.requires_grad = False\n",
    "\n",
    "        h = self.lm.config.hidden_size\n",
    "        self.head = nn.Linear(h, 1)  # 1-logit\n",
    "\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None,\n",
    "                rw_mask=None, rewards=None):\n",
    "        out = self.lm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        H = out.hidden_states[-1]       # (bs, seq, h)\n",
    "        logit = self.head(H).squeeze(-1)  # (bs, seq)\n",
    "\n",
    "        loss = None\n",
    "        if rewards is not None and rw_mask is not None:\n",
    "            m = rw_mask.bool()\n",
    "            if m.ndim == 2:   # (bs, seq)\n",
    "                sel_logit = logit[m]\n",
    "                sel_label = rewards[m].float()\n",
    "            else:             # (seq,) 단일 배치 보호\n",
    "                sel_logit = logit[m]\n",
    "                sel_label = rewards[m].float()\n",
    "            if sel_logit.numel() > 0:\n",
    "                loss = self.loss_fn(sel_logit, sel_label)\n",
    "            else:\n",
    "                # <RW>가 잘려서 없는 샘플은 로스 없음(=0으로 무시)\n",
    "                loss = torch.zeros([], dtype=logit.dtype, device=logit.device)\n",
    "\n",
    "        return {\"loss\": loss, \"logits_prm\": logit, \"hidden_states\": out.hidden_states}\n",
    "\n",
    "@dataclass\n",
    "class CollatedBatch:\n",
    "    input_ids: torch.Tensor\n",
    "    attention_mask: torch.Tensor\n",
    "    rw_mask: torch.Tensor\n",
    "    rewards: torch.Tensor\n",
    "\n",
    "def default_collate(batch: list[dict]) -> CollatedBatch:\n",
    "    # 모두 동일 길이(padding=\"max_length\")라면 stack만 해도 됨\n",
    "    keys = [\"input_ids\",\"attention_mask\",\"rw_mask\",\"rewards\"]\n",
    "    out: Dict[str, Any] = {}\n",
    "    for k in keys:\n",
    "        out[k] = torch.stack([b[k] for b in batch], dim=0)\n",
    "    return CollatedBatch(**out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 토크나이저/엔트리 준비\n",
    "base = \"mistral-community/Mistral-7B-v0.2\"   # 또는 Qwen/Mistral base\n",
    "tok  = AutoTokenizer.from_pretrained(base, trust_remote_code=True)\n",
    "\n",
    "# (중요) pad_token 보정\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# (예시) entries 골격: 각 entry는 step list와 같은 개수의 보상 리스트 포함\n",
    "entries = [\n",
    "    {\n",
    "        \"question\": \"Janet’s ducks lay 16 eggs per day ...\",\n",
    "        \"completion\": [\n",
    "            \"Step 1: ...\",\n",
    "            \"Step 2: ...\",\n",
    "            \"Step 3: ...\",\n",
    "            \"Step 4: ...\",\n",
    "        ],\n",
    "        \"mi_loo\": [0.9, 0.8, 0.95, 0.1],\n",
    "        \"contributions\": [0.7, 0.6, 0.8, 0.2],\n",
    "        \"mi_filtered\": [0.1, 0.1, 0.1, 0.0],\n",
    "        \"ori_rewards\": [1,1,1,0],\n",
    "        \"gold_answer\": \"18\",\n",
    "    },\n",
    "    # ...\n",
    "]\n",
    "\n",
    "# 2) 데이터셋 생성\n",
    "train_ds = StepwisePRMWithRW(\n",
    "    entries, tokenizer=tok,\n",
    "    max_length=2048,\n",
    "    reward_type=\"mi_loo\",    # 네 파이프라인에 맞게 선택\n",
    "    preprocess=True,\n",
    "    apply_norm=True, norm_mode=\"unit\",\n",
    "    norm_kwargs={\"tau\":1.5, \"clip_z\":3.0, \"deadzone\":0.2, \"q_low\":5, \"q_high\":95, \"round_to\":4},\n",
    ")\n",
    "\n",
    "# 3) 모델/트레이너\n",
    "model = PRMTokenClassifier(base, tokenizer=tok, use_lora=False, freeze_backbone=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"prm_rw_out\",\n",
    "    learning_rate=1e-5,                 # 7B 기준 5e-6 ~ 2e-5 탐색 추천\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    bf16=True, fp16=False,\n",
    "    logging_steps=10,\n",
    "    save_steps=1000,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "class PRMTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        out = model(**inputs.__dict__)\n",
    "        loss = out[\"loss\"]\n",
    "        return (loss, out) if return_outputs else loss\n",
    "\n",
    "trainer = PRMTrainer(\n",
    "    model=model, args=args,\n",
    "    train_dataset=train_ds,\n",
    "    data_collator=default_collate,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def score_sample(model: PRMTokenClassifier, tok: AutoTokenizer, text: str, max_length=2048):\n",
    "    enc = tok(text + f\"\\n{RW_TOKEN}\", return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    input_ids = enc[\"input_ids\"].to(model.lm.device)\n",
    "    attn      = enc[\"attention_mask\"].to(model.lm.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attn)\n",
    "        logit = out[\"logits_prm\"]  # (1, seq)\n",
    "    rw_id = tok.convert_tokens_to_ids(RW_TOKEN)\n",
    "    rw_mask = (input_ids[0] == rw_id)\n",
    "    s = logit[0][rw_mask]\n",
    "    return torch.sigmoid(s).cpu().tolist()\n",
    "\n",
    "# 예시\n",
    "# prob = score_sample(model, tok, \"System...\\nProblem: ...\\nStep 1 ...\\nStep 2 ...\")\n",
    "# print(prob)  # [0.83]  (해당 샘플의 스텝 보상)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
